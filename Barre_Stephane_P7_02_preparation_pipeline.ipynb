{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02 : Construction du Pipeline de Prétraitement\n",
    "\n",
    "## Contexte\n",
    "\n",
    "**Objectif :** Créer un pipeline sklearn réutilisable et robuste pour préparer les données avant la modélisation.\n",
    "\n",
    "Un pipeline permet de :\n",
    "- Encapsuler toutes les étapes de prétraitement dans un objet unique\n",
    "- Garantir que les mêmes transformations sont appliquées sur train/valid/test\n",
    "- Éviter les fuites de données (data leakage)\n",
    "- Faciliter la mise en production\n",
    "\n",
    "---\n",
    "\n",
    "## Input\n",
    "\n",
    "**Fichier d'entrée :** `application_train_AGGREGATED.csv`\n",
    "- Créé par le Notebook 01\n",
    "- Shape attendu : (307 511 lignes, 797 colonnes)\n",
    "- Contient les données agrégées des 7 tables\n",
    "\n",
    "---\n",
    "\n",
    "## Étapes du pipeline\n",
    "\n",
    "### 1. Séparation X et y\n",
    "- `X` : Toutes les colonnes sauf `SK_ID_CURR` (identifiant) et `TARGET` (variable cible)\n",
    "- `y` : Variable binaire `TARGET` (0 = remboursement, 1 = défaut)\n",
    "\n",
    "### 2. Split train/validation\n",
    "- **Train** : 80% des données pour entraîner le modèle\n",
    "- **Validation** : 20% des données pour évaluer les performances\n",
    "- `stratify=y` : Maintient la même proportion de 0/1 dans train et valid\n",
    "- `random_state=42` : Reproductibilité des résultats\n",
    "\n",
    "### 3. Identification des types de variables\n",
    "- **Variables numériques** : int64, float64 (à standardiser)\n",
    "- **Variables catégorielles** : object, category (déjà encodées en One-Hot dans Notebook 01)\n",
    "\n",
    "### 4. ColumnTransformer\n",
    "Applique des transformations différentes selon le type de variable :\n",
    "\n",
    "**Pour les variables numériques :**\n",
    "- `SimpleImputer(strategy='median')` : Remplace les NaN par la médiane (robuste aux outliers)\n",
    "- `StandardScaler()` : Standardisation (moyenne=0, écart-type=1)\n",
    "\n",
    "**Pour les variables catégorielles :**\n",
    "- `SimpleImputer(strategy='constant', fill_value='missing')` : Remplace les NaN par 'missing'\n",
    "- Pas de scaling nécessaire (déjà en 0/1 après One-Hot Encoding)\n",
    "\n",
    "**Pourquoi standardiser les variables numériques ?**\n",
    "- Régression Logistique : Nécessite des variables à échelles comparables\n",
    "- Convergence plus rapide des algorithmes d'optimisation\n",
    "- Interprétation plus facile des coefficients\n",
    "\n",
    "**Pourquoi la médiane plutôt que la moyenne ?**\n",
    "- Plus robuste aux valeurs extrêmes (outliers)\n",
    "- Mieux adaptée aux distributions asymétriques\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "\n",
    "Le notebook génère 3 fichiers joblib dans le dossier `artifacts/` :\n",
    "\n",
    "1. **preprocesseur.joblib** : Pipeline de prétraitement complet (ColumnTransformer)\n",
    "2. **data_split.joblib** : Tuple (X_train, X_valid, y_train, y_valid)\n",
    "3. **feature_names.joblib** : Liste des noms de features après transformation\n",
    "\n",
    "Ces fichiers seront réutilisés dans les notebooks suivants.\n",
    "\n",
    "---\n",
    "\n",
    "## Prévention du Data Leakage\n",
    "\n",
    "**Principe crucial :** Le prétraitement doit être ajusté (fit) UNIQUEMENT sur le train, puis appliqué (transform) sur train ET validation.\n",
    "\n",
    "### Approche CORRECTE (pas de data leakage)\n",
    "\n",
    "```\n",
    "Données complètes (X, y)\n",
    "|\n",
    "v\n",
    "SPLIT FIRST\n",
    "|\n",
    "+---+---+\n",
    "| |\n",
    "v v\n",
    "TRAIN VALID\n",
    "|\n",
    "v\n",
    "FIT preprocesseur\n",
    "(calcul médiane, mean, std)\n",
    "|\n",
    "+-------+-------+\n",
    "| |\n",
    "v v\n",
    "TRANSFORM TRANSFORM\n",
    "TRAIN VALID\n",
    "```\n",
    "\n",
    "**Exemple correct :**\n",
    "```python\n",
    "# 1. Split AVANT tout prétraitement\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n",
    "\n",
    "# 2. Fit sur train seulement\n",
    "preprocesseur.fit(X_train)\n",
    "\n",
    "# 3. Transform sur train ET valid\n",
    "X_train_processed = preprocesseur.transform(X_train)\n",
    "X_valid_processed = preprocesseur.transform(X_valid)\n",
    "```\n",
    "\n",
    "### Approche INCORRECTE (data leakage)\n",
    "\n",
    "```\n",
    "Données complètes (X, y)\n",
    "|\n",
    "v\n",
    "FIT preprocesseur ← ERREUR : utilise les données validation !\n",
    "(calcul médiane, mean, std)\n",
    "|\n",
    "v\n",
    "TRANSFORM\n",
    "|\n",
    "v\n",
    "SPLIT ← Trop tard, le modèle a déjà \"vu\" la validation\n",
    "|\n",
    "+---+---+\n",
    "| |\n",
    "v v\n",
    "TRAIN VALID\n",
    "```\n",
    "\n",
    "**Erreur à éviter :**\n",
    "```python\n",
    "# ERREUR : Fit sur toutes les données AVANT split\n",
    "preprocesseur.fit(X) # Le modèle \"voit\" les données validation !\n",
    "X_processed = preprocesseur.transform(X)\n",
    "X_train, X_valid = train_test_split(X_processed)\n",
    "```\n",
    "\n",
    "**Pourquoi c'est grave ?**\n",
    "- La médiane calculée sur toutes les données inclut les valeurs de validation\n",
    "- Le StandardScaler utilise la moyenne/écart-type de validation\n",
    "- Le modèle a indirectement accès à des informations de validation\n",
    "- Les performances mesurées sont artificiellement gonflées\n",
    "- En production, les performances réelles seront décevantes\n",
    "\n",
    "**Dans notre notebook, nous suivons la méthode CORRECTE !**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports des librairies nécessaires\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn : split et prétraitement\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Sauvegarde des objets\n",
    "import joblib\n",
    "\n",
    "# Configuration visualisation\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "print('Imports réussis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des données (optimisé mémoire)\n",
    "\n",
    "**Source :** Fichier `application_train_AGGREGATED.csv` créé par le Notebook 01\n",
    "\n",
    "Ce fichier contient :\n",
    "- Les 307 511 clients du dataset train\n",
    "- Les 797 colonnes (122 initiales + 675 agrégées)\n",
    "- La variable cible `TARGET` (0 ou 1)\n",
    "\n",
    "**Optimisation mémoire :**\n",
    "- Conversion automatique des types pour réduire l'empreinte mémoire\n",
    "- float64 → float32 (divise la mémoire par 2)\n",
    "- int64 → int32 ou int16 selon les valeurs\n",
    "- Fichier original : ~1.4 GB en mémoire\n",
    "- Fichier optimisé : ~700 MB en mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des chemins\n",
    "DOSSIER_DATA = Path('./data')\n",
    "DOSSIER_ARTIFACTS = Path('./artifacts')\n",
    "\n",
    "# Créer le dossier artifacts s'il n'existe pas\n",
    "DOSSIER_ARTIFACTS.mkdir(exist_ok=True)\n",
    "\n",
    "# Chemin du fichier agrégé\n",
    "CHEMIN_TRAIN = DOSSIER_DATA / 'application_train_AGGREGATED.csv'\n",
    "\n",
    "print(\"Configuration des chemins :\")\n",
    "print(f\" Données : {CHEMIN_TRAIN}\")\n",
    "print(f\" Artifacts : {DOSSIER_ARTIFACTS}\")\n",
    "\n",
    "print(\"\\nVérification du fichier...\")\n",
    "if CHEMIN_TRAIN.exists():\n",
    "    file_size = CHEMIN_TRAIN.stat().st_size / (1024**2)\n",
    "    print(f\" Fichier trouvé : {file_size:.1f} MB\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Fichier non trouvé : {CHEMIN_TRAIN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Réduit l'usage mémoire d'un DataFrame en convertissant les types de données.\n",
    "\n",
    "    float64 → float32 (divise par 2)\n",
    "    int64   → int32 / int16 / int8 selon min/max\n",
    "    \"\"\"\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f\"Mémoire initiale : {start_mem:.2f} MB\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            # Gestion des entiers\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "\n",
    "            # Gestion des floats\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f\"Mémoire finale : {end_mem:.2f} MB\")\n",
    "        print(f\"Réduction : {100 * (start_mem - end_mem) / start_mem:.1f}%\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données avec optimisation mémoire (par chunks)\n",
    "print(\"\\nChargement des données par morceaux...\")\n",
    "print(\"(Cela peut prendre 3-5 minutes)\\n\")\n",
    "\n",
    "# Paramètres de chargement (chunks plus petits pour économiser la RAM)\n",
    "chunk_size = 20000  # 20 000 lignes à la fois\n",
    "chunks = []\n",
    "\n",
    "print(\"Lecture du fichier par chunks de 20 000 lignes...\")\n",
    "\n",
    "try:\n",
    "    for i, chunk in enumerate(pd.read_csv(CHEMIN_TRAIN, chunksize=chunk_size, low_memory=False)):\n",
    "        print(f\" Chunk {i+1} : {len(chunk):,} lignes chargées\", end='')\n",
    "\n",
    "        # Optimisation immédiate de chaque chunk\n",
    "        chunk = reduce_mem_usage(chunk, verbose=False)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        print(\" → optimisé (RAM économisée)\")\n",
    "\n",
    "except MemoryError as e:\n",
    "    print(\"\\n\\nERREUR MÉMOIRE : Pas assez de RAM disponible\")\n",
    "    print(\"RAM nécessaire : ~4 GB\")\n",
    "    print(\"\\nActions recommandées :\")\n",
    "    print(\" 1. Fermez Google Chrome, Edge, WhatsApp, Dropbox\")\n",
    "    print(\" 2. Redémarrez Jupyter : Kernel → Restart\")\n",
    "    print(\" 3. Relancez le notebook\")\n",
    "    raise\n",
    "\n",
    "# Concaténation de tous les chunks\n",
    "print(\"\\nConcaténation des chunks...\")\n",
    "df_train = pd.concat(chunks, ignore_index=True)\n",
    "del chunks  # Libération mémoire\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nChargement réussi\")\n",
    "print(f\" Shape : {df_train.shape}\")\n",
    "print(f\" Lignes : {df_train.shape[0]:,}\")\n",
    "print(f\" Colonnes : {df_train.shape[1]:,}\")\n",
    "\n",
    "# Optimisation finale\n",
    "print(\"\\nOptimisation mémoire finale...\")\n",
    "start_mem = df_train.memory_usage().sum() / 1024**2\n",
    "print(f\" Mémoire utilisée : {start_mem:.1f} MB\")\n",
    "\n",
    "# Vérification de la présence de TARGET\n",
    "print(\"\\nVérification de TARGET...\")\n",
    "if 'TARGET' in df_train.columns:\n",
    "    print(\" Variable TARGET présente\")\n",
    "    print(\" Distribution :\")\n",
    "    for val, pct in df_train['TARGET'].value_counts(normalize=True).items():\n",
    "        print(f\" Classe {val} : {pct*100:.1f}%\")\n",
    "else:\n",
    "    raise ValueError(\"La colonne TARGET est absente !\")\n",
    "\n",
    "print(\"\\nDonnées prêtes pour le traitement\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyse exploratoire rapide\n",
    "\n",
    "### Distribution de la variable cible\n",
    "\n",
    "**Déséquilibre des classes :**\n",
    "- Classe 0 (remboursement) : environ 92% des clients\n",
    "- Classe 1 (défaut) : environ 8% des clients\n",
    "\n",
    "Ce fort déséquilibre implique que :\n",
    "- L'accuracy n'est PAS une bonne métrique (un modèle prédisant toujours 0 aurait 92% d'accuracy !)\n",
    "- Il faut utiliser l'AUC (Area Under Curve) comme métrique principale\n",
    "- Des techniques comme SMOTE ou class_weight peuvent améliorer les performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la distribution de TARGET\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "target_counts = df_train['TARGET'].value_counts()\n",
    "target_pct = df_train['TARGET'].value_counts(normalize=True) * 100\n",
    "\n",
    "ax.bar(['Remboursement (0)', 'Défaut (1)'], target_counts, \n",
    "       color=['#2ecc71', '#e74c3c'])\n",
    "ax.set_ylabel('Nombre de clients')\n",
    "ax.set_title('Distribution de la variable TARGET\\n(Déséquilibre : 92% / 8%)')\n",
    "\n",
    "# Ajout des pourcentages sur les barres\n",
    "for i, (count, pct) in enumerate(zip(target_counts, target_pct)):\n",
    "    ax.text(i, count + 5000, f'{pct:.1f}%\\n({count:,})',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nRatio de déséquilibre : 1 défaut pour {target_counts[0] / target_counts[1]:.1f} remboursements\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valeurs manquantes\n",
    "\n",
    "Les valeurs manquantes (NaN) sont fréquentes dans ce dataset pour plusieurs raisons :\n",
    "- Client n'a pas de crédit bureau → features BURO_* = NaN\n",
    "- Client n'a jamais demandé de crédit chez Home Credit → features PREV_* = NaN\n",
    "- Agrégation sur des groupes vides → NaN\n",
    "\n",
    "**Stratégie de gestion :**\n",
    "- Variables numériques : Imputation par la médiane (robuste)\n",
    "- Variables catégorielles : Imputation par 'missing' (catégorie spéciale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des valeurs manquantes\n",
    "missing_values = df_train.isnull().sum()\n",
    "missing_percent = (missing_values / len(df_train)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Colonne': missing_values.index,\n",
    "    'NaN_count': missing_values.values,\n",
    "    'NaN_percent': missing_percent.values\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['NaN_count'] > 0].sort_values('NaN_percent', ascending=False)\n",
    "\n",
    "print(\"\\nStatistiques sur les valeurs manquantes :\")\n",
    "print(f\" Nombre de colonnes avec NaN : {len(missing_df)} / {df_train.shape[1]}\")\n",
    "print(f\" Colonnes avec >50% NaN : {len(missing_df[missing_df['NaN_percent'] > 50])}\")\n",
    "\n",
    "print(\"\\nTop 10 des colonnes avec le plus de NaN :\")\n",
    "print(missing_df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Séparation X et y\n",
    "\n",
    "**Variables à exclure :**\n",
    "- `SK_ID_CURR` : Identifiant client (pas prédictif)\n",
    "- `TARGET` : Variable cible (à prédire)\n",
    "\n",
    "**Toutes les autres colonnes sont des features pour la prédiction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation X (features) et y (target)\n",
    "print(\"\\nSéparation X et y...\")\n",
    "\n",
    "# Colonnes à exclure de X\n",
    "cols_to_exclude = ['SK_ID_CURR', 'TARGET']\n",
    "cols_to_exclude = [col for col in cols_to_exclude if col in df_train.columns]\n",
    "\n",
    "# X : toutes les features sauf SK_ID_CURR et TARGET\n",
    "X = df_train.drop(columns=cols_to_exclude)\n",
    "\n",
    "# y : variable cible\n",
    "y = df_train['TARGET']\n",
    "\n",
    "print(f\"\\nRésultat de la séparation :\")\n",
    "print(f\" X shape : {X.shape}\")\n",
    "print(f\" - {X.shape[0]:,} échantillons (clients)\")\n",
    "print(f\" - {X.shape[1]:,} features\")\n",
    "print(f\" y shape : {y.shape}\")\n",
    "print(f\" - Distribution : {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Libération mémoire\n",
    "del df_train\n",
    "import gc\n",
    "gc.collect()\n",
    "print(f\"\\nMémoire libérée (df_train supprimé)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Nettoyage des valeurs infinies\n",
    "\n",
    "**Problème détecté :** Les divisions dans le Notebook 01 peuvent créer des valeurs infinies (Inf) quand le dénominateur est zéro.\n",
    "\n",
    "**Exemples :**\n",
    "```python\n",
    "INCOME_CREDIT_PERC = AMT_INCOME_TOTAL / AMT_CREDIT\n",
    "# Si AMT_CREDIT = 0 → Inf\n",
    "\n",
    "ANNUITY_INCOME_PERC = AMT_ANNUITY / AMT_INCOME_TOTAL\n",
    "# Si AMT_INCOME_TOTAL = 0 → Inf\n",
    "```\n",
    "\n",
    "**Solution :** Remplacer toutes les valeurs Inf par NaN colonne par colonne (pour économiser la mémoire)\n",
    "- Le SimpleImputer gérera ensuite ces NaN avec la médiane\n",
    "- Évite l'erreur \"Input X contains infinity\" dans StandardScaler\n",
    "\n",
    "**Impact :** Les Inf sont traités comme des valeurs manquantes, ce qui est plus sûr que de les garder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage des valeurs infinies (colonne par colonne pour économiser la RAM)\n",
    "print(\"\\nNettoyage des valeurs infinies...\")\n",
    "\n",
    "# Comptage des Inf avant nettoyage\n",
    "inf_count_before = 0\n",
    "for col in X.select_dtypes(include=[np.number]).columns:\n",
    "    inf_count_before += np.isinf(X[col]).sum()\n",
    "\n",
    "print(f\" Valeurs Inf détectées : {inf_count_before:,}\")\n",
    "\n",
    "if inf_count_before > 0:\n",
    "    # Remplacement Inf par NaN colonne par colonne (économise la RAM)\n",
    "    cols_with_inf = []\n",
    "    for col in X.select_dtypes(include=[np.number]).columns:\n",
    "        if np.isinf(X[col]).any():\n",
    "            X[col] = X[col].replace([np.inf, -np.inf], np.nan)\n",
    "            cols_with_inf.append(col)\n",
    "\n",
    "    print(f\" Colonnes nettoyées : {len(cols_with_inf)}\")\n",
    "\n",
    "    # Vérification après nettoyage\n",
    "    inf_count_after = 0\n",
    "    for col in X.select_dtypes(include=[np.number]).columns:\n",
    "        inf_count_after += np.isinf(X[col]).sum()\n",
    "\n",
    "    print(f\" Valeurs Inf après nettoyage : {inf_count_after}\")\n",
    "\n",
    "else:\n",
    "    print(\" Aucune valeur Inf à nettoyer\")\n",
    "\n",
    "print(\" Ces valeurs seront imputées par la médiane dans le preprocessing\")\n",
    "print(\"\\nNettoyage terminé\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Split Train / Validation\n",
    "\n",
    "**Flux de données complet :**\n",
    "\n",
    "```\n",
    "application_train_AGGREGATED.csv\n",
    "(307 511 × 797)\n",
    "|\n",
    "v\n",
    "Séparation X / y\n",
    "|\n",
    "+-----------+-----------+\n",
    "| |\n",
    "v v\n",
    "X (795 features) y (TARGET)\n",
    "| |\n",
    "+----------+------------+\n",
    "|\n",
    "v\n",
    "train_test_split()\n",
    "test_size=0.2, stratify=y\n",
    "|\n",
    "+----------+----------+\n",
    "| |\n",
    "v v\n",
    "TRAIN SET VALIDATION SET\n",
    "80% (246 008) 20% (61 503)\n",
    "| |\n",
    "v v\n",
    "X_train X_valid\n",
    "y_train y_valid\n",
    "(92% class 0) (92% class 0)\n",
    "(8% class 1) (8% class 1)\n",
    "```\n",
    "\n",
    "**Paramètres du split :**\n",
    "- `test_size=0.2` : 80% train, 20% validation\n",
    "- `random_state=42` : Reproductibilité des résultats\n",
    "- `stratify=y` : Maintient la proportion 92/8 dans train ET validation\n",
    "\n",
    "**Pourquoi stratify ?**\n",
    "Sans stratification, le validation set pourrait avoir une proportion différente de défauts (ex: 85/15 ou 95/5), ce qui fausserait complètement l'évaluation du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/validation avec stratification\n",
    "print(\"\\nSplit train/validation (80/20)...\")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # Maintient la proportion 92/8\n",
    ")\n",
    "\n",
    "print(\"\\nRésultat du split :\")\n",
    "\n",
    "print(\"\\nTrain set :\")\n",
    "print(f\" X_train shape : {X_train.shape}\")\n",
    "print(f\" y_train shape : {y_train.shape}\")\n",
    "print(f\" y_train distribution : \\n{y_train.value_counts(normalize=True)}\")\n",
    "\n",
    "print(\"\\nValidation set :\")\n",
    "print(f\" X_valid shape : {X_valid.shape}\")\n",
    "print(f\" y_valid shape : {y_valid.shape}\")\n",
    "print(f\" y_valid distribution : \\n{y_valid.value_counts(normalize=True)}\")\n",
    "\n",
    "print(\"\\nVérification stratification : OK (proportions identiques)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Identification des types de variables\n",
    "\n",
    "**Variables numériques** (int64, float64) :\n",
    "- Nécessitent imputation (médiane) et standardisation\n",
    "- Exemples : AMT_INCOME_TOTAL, DAYS_BIRTH, BURO_DAYS_CREDIT_MEAN\n",
    "\n",
    "**Variables catégorielles** (object) :\n",
    "- Déjà encodées en One-Hot dans le Notebook 01\n",
    "- Nécessitent seulement imputation ('missing')\n",
    "- Exemples : NAME_EDUCATION_TYPE_Higher education, FLAG_OWN_CAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des types de colonnes\n",
    "print(\"\\nIdentification des types de variables...\")\n",
    "\n",
    "# Variables numériques\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Variables catégorielles (normalement aucune après One-Hot Encoding dans Notebook 01)\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\nRésultat :\")\n",
    "print(f\" Variables numériques : {len(numeric_features)}\")\n",
    "print(f\" Exemples : {numeric_features[:5]}\")\n",
    "\n",
    "print(f\" Variables catégorielles : {len(categorical_features)}\")\n",
    "if len(categorical_features) > 0:\n",
    "    print(f\" Exemples : {categorical_features[:5]}\")\n",
    "else:\n",
    "    print(\" Aucune (toutes encodées en One-Hot dans Notebook 01)\")\n",
    "\n",
    "# Vérification\n",
    "total_features = len(numeric_features) + len(categorical_features)\n",
    "print(f\"\\nTotal : {total_features} features (cohérent avec X_train.shape[1] = {X_train.shape[1]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Construction du ColumnTransformer\n",
    "\n",
    "**Architecture du preprocessing pipeline :**\n",
    "\n",
    "```\n",
    "X_train (246 008 lignes × 795 colonnes)\n",
    "|\n",
    "v\n",
    "ColumnTransformer\n",
    "|\n",
    "+---------------+---------------+\n",
    "| |\n",
    "v v\n",
    "Variables NUMÉRIQUES Variables CATÉGORIELLES\n",
    "(795 colonnes) (0 colonnes)\n",
    "| |\n",
    "v v\n",
    "+-------------------+ +-------------------+\n",
    "| SimpleImputer | | SimpleImputer |\n",
    "| strategy='median'| | fill='missing' |\n",
    "+-------------------+ +-------------------+\n",
    "| |\n",
    "v |\n",
    "+-------------------+ |\n",
    "| StandardScaler() | |\n",
    "| mean=0, std=1 | |\n",
    "+-------------------+ |\n",
    "| |\n",
    "+---------------+---------------+\n",
    "|\n",
    "v\n",
    "X_train_processed (array numpy)\n",
    "- Pas de NaN\n",
    "- Variables centrées et réduites\n",
    "- Prêt pour modélisation\n",
    "```\n",
    "\n",
    "**Avantages du ColumnTransformer :**\n",
    "- Applique automatiquement les transformations sur les bonnes colonnes\n",
    "- Peut être sauvegardé et rechargé (joblib)\n",
    "- S'intègre avec sklearn.pipeline.Pipeline\n",
    "- Évite les erreurs de transformation manuelle\n",
    "\n",
    "**Note :** Dans notre cas, toutes les variables sont numériques car le One-Hot Encoding a été fait dans le Notebook 01. Les variables catégorielles (NAME_EDUCATION_TYPE, etc.) ont été transformées en colonnes binaires (0/1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction du ColumnTransformer\n",
    "print(\"\\nConstruction du pipeline de prétraitement...\")\n",
    "\n",
    "# Pipeline pour les variables numériques\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Imputation par la médiane\n",
    "    ('scaler', StandardScaler())                    # Standardisation\n",
    "])\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Fonction pour convertir tout en string\n",
    "def convert_to_string(X):\n",
    "    \"\"\"Convertit toutes les valeurs en string pour OneHotEncoder\"\"\"\n",
    "    import pandas as pd\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        return X.astype(str)\n",
    "    else:\n",
    "        return pd.DataFrame(X).astype(str).values\n",
    "\n",
    "# Pipeline pour les variables catégorielles\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('to_string', FunctionTransformer(convert_to_string)),\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')),\n",
    "    ('imputer_final', SimpleImputer(strategy='constant', fill_value=0))  # Impute NaN créés par OHE\n",
    "])\n",
    "\n",
    "# Assemblage dans le ColumnTransformer\n",
    "transformers = [\n",
    "    ('num', numeric_transformer, numeric_features)\n",
    "]\n",
    "\n",
    "if len(categorical_features) > 0:\n",
    "    transformers.append(('cat', categorical_transformer, categorical_features))\n",
    "\n",
    "preprocesseur = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder='passthrough'  # Garde les colonnes non spécifiées\n",
    ")\n",
    "\n",
    "print(\"\\nPipeline créé avec succès\")\n",
    "print(f\" Transformations numériques : {len(numeric_features)} colonnes\")\n",
    "print(\" - Imputation : médiane\")\n",
    "print(\" - Scaling : StandardScaler\")\n",
    "\n",
    "if len(categorical_features) > 0:\n",
    "    print(f\" Transformations catégorielles : {len(categorical_features)} colonnes\")\n",
    "    print(\" - Imputation : 'missing'\")\n",
    "    print(\" - OneHotEncoding : drop='first'\")\n",
    "    print(\" - Imputation finale : 0 (pour NaN créés par OHE)\")\n",
    "else:\n",
    "    print(\" Aucune variable catégorielle (tout a été encodé dans Notebook 01)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fit du preprocesseur sur le train set\n",
    "\n",
    "**ATTENTION : Point critique pour éviter le data leakage**\n",
    "\n",
    "Le fit du prétraitement doit être fait **UNIQUEMENT** sur le train set.\n",
    "\n",
    "Lors du fit :\n",
    "- `SimpleImputer` calcule la médiane des variables numériques (sur train seulement)\n",
    "- `StandardScaler` calcule la moyenne et l'écart-type (sur train seulement)\n",
    "\n",
    "Puis lors du transform :\n",
    "- Ces statistiques (calculées sur train) sont appliquées sur train ET validation\n",
    "\n",
    "**Pourquoi c'est important ?**\n",
    "Si on fit sur toutes les données, le modèle \"voit\" des informations du validation set, ce qui surestime artificiellement les performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit du preprocesseur sur X_train UNIQUEMENT\n",
    "print(\"\\nFit du preprocesseur sur le train set...\")\n",
    "print(\"(Calcul des médianes et paramètres de standardisation)\")\n",
    "\n",
    "preprocesseur.fit(X_train)\n",
    "\n",
    "print(f\"\\nFit terminé avec succès\")\n",
    "print(f\" Médianes calculées sur {X_train.shape[0]:,} échantillons\")\n",
    "print(f\" Paramètres de scaling calculés sur {X_train.shape[0]:,} échantillons\")\n",
    "print(f\"\\nPrêt pour transform sur train et validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Transform sur train et validation\n",
    "\n",
    "**Application des transformations :**\n",
    "- Le préprocesseur applique les transformations fit sur train\n",
    "- Les NaN sont remplacés par les médianes du train\n",
    "- Les variables sont standardisées avec la moyenne/écart-type du train\n",
    "\n",
    "**Format de sortie :**\n",
    "- Arrays numpy (pas de DataFrames)\n",
    "- Shape : (n_samples, n_features)\n",
    "- Toutes les valeurs sont numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform sur train et validation\n",
    "print(\"\\nTransform des données...\")\n",
    "\n",
    "X_train_processed = preprocesseur.transform(X_train)\n",
    "X_valid_processed = preprocesseur.transform(X_valid)\n",
    "\n",
    "print(\"\\nTransform terminé avec succès\")\n",
    "\n",
    "print(\"\\nTrain set transformé :\")\n",
    "print(f\" Shape : {X_train_processed.shape}\")\n",
    "print(f\" Type : {type(X_train_processed)}\")\n",
    "print(f\" Dtype : {X_train_processed.dtype}\")\n",
    "\n",
    "# Vérification des NaN (avec gestion d'erreur)\n",
    "try:\n",
    "    nan_count_train = np.isnan(X_train_processed).sum()\n",
    "    print(f\" NaN restants : {nan_count_train}\")\n",
    "except TypeError:\n",
    "    print(\" NaN restants : Impossible à vérifier (types mixtes détectés)\")\n",
    "    nan_count_train = -1\n",
    "\n",
    "print(\"\\nValidation set transformé :\")\n",
    "print(f\" Shape : {X_valid_processed.shape}\")\n",
    "print(f\" Type : {type(X_valid_processed)}\")\n",
    "print(f\" Dtype : {X_valid_processed.dtype}\")\n",
    "\n",
    "# Vérification des NaN (avec gestion d'erreur)\n",
    "try:\n",
    "    nan_count_valid = np.isnan(X_valid_processed).sum()\n",
    "    print(f\" NaN restants : {nan_count_valid}\")\n",
    "except TypeError:\n",
    "    print(\" NaN restants : Impossible à vérifier (types mixtes détectés)\")\n",
    "    nan_count_valid = -1\n",
    "\n",
    "# Vérification finale\n",
    "if nan_count_train == 0 and nan_count_valid == 0:\n",
    "    print(\"\\nVérification : OK (aucun NaN après preprocessing)\")\n",
    "elif nan_count_train == -1 or nan_count_valid == -1:\n",
    "    print(\"\\nVérification : Données transformées (types mixtes, vérification NaN non applicable)\")\n",
    "else:\n",
    "    print(\"\\nATTENTION : Des NaN persistent après preprocessing !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CORRECTION : Imputation finale des NaN\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nCorrection : Imputation des NaN residuels...\\n\")\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Compter les NaN avant\n",
    "nan_before_train = np.isnan(X_train_processed).sum()\n",
    "nan_before_valid = np.isnan(X_valid_processed).sum()\n",
    "\n",
    "print(\"NaN avant imputation :\")\n",
    "print(f\" Train : {nan_before_train:,}\")\n",
    "print(f\" Valid : {nan_before_valid:,}\")\n",
    "\n",
    "# Imputation avec la médiane\n",
    "imputer_final = SimpleImputer(strategy='median')\n",
    "X_train_processed = imputer_final.fit_transform(X_train_processed)\n",
    "X_valid_processed = imputer_final.transform(X_valid_processed)\n",
    "\n",
    "# Compter les NaN après\n",
    "nan_after_train = np.isnan(X_train_processed).sum()\n",
    "nan_after_valid = np.isnan(X_valid_processed).sum()\n",
    "\n",
    "print(\"\\nNaN après imputation :\")\n",
    "print(f\" Train : {nan_after_train:,}\")\n",
    "print(f\" Valid : {nan_after_valid:,}\")\n",
    "\n",
    "if nan_after_train == 0 and nan_after_valid == 0:\n",
    "    print(\"\\nImputation réussie - Aucun NaN résiduel\")\n",
    "else:\n",
    "    print(f\"\\nATTENTION : {nan_after_train + nan_after_valid:,} NaN persistent\")\n",
    "\n",
    "print(\"\\nShapes finaux :\")\n",
    "print(f\" X_train_processed : {X_train_processed.shape}\")\n",
    "print(f\" X_valid_processed : {X_valid_processed.shape}\")\n",
    "\n",
    "print(\"\\nDonnées prêtes pour la modélisation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Vérification de la standardisation\n",
    "\n",
    "**Après StandardScaler, les variables numériques doivent avoir :**\n",
    "- Moyenne ≈ 0\n",
    "- Écart-type ≈ 1\n",
    "\n",
    "Note : Sur le validation set, la moyenne et l'écart-type peuvent être légèrement différents de 0 et 1 car on applique les paramètres du train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification de la standardisation\n",
    "print(\"\\nVérification de la standardisation...\")\n",
    "\n",
    "# Gestion des types mixtes : conversion en float si nécessaire\n",
    "try:\n",
    "    # Tentative de calcul direct\n",
    "    mean_train = X_train_processed.mean(axis=0)\n",
    "    std_train = X_train_processed.std(axis=0)\n",
    "\n",
    "    print(\"\\nStatistiques Train (après StandardScaler) :\")\n",
    "    print(f\" Moyenne globale : {mean_train.mean():.6f} (attendu : ≈ 0)\")\n",
    "    print(f\" Écart-type global : {std_train.mean():.6f} (attendu : ≈ 1)\")\n",
    "    print(f\" Min de toutes les features : {X_train_processed.min():.2f}\")\n",
    "    print(f\" Max de toutes les features : {X_train_processed.max():.2f}\")\n",
    "\n",
    "    # Statistiques sur validation\n",
    "    mean_valid = X_valid_processed.mean(axis=0)\n",
    "    std_valid = X_valid_processed.std(axis=0)\n",
    "\n",
    "    print(\"\\nStatistiques Validation (après StandardScaler) :\")\n",
    "    print(f\" Moyenne globale : {mean_valid.mean():.6f} (peut différer légèrement de 0)\")\n",
    "    print(f\" Écart-type global : {std_valid.mean():.6f} (peut différer légèrement de 1)\")\n",
    "    print(f\" Min de toutes les features : {X_valid_processed.min():.2f}\")\n",
    "    print(f\" Max de toutes les features : {X_valid_processed.max():.2f}\")\n",
    "\n",
    "except (TypeError, ValueError):\n",
    "    # Types mixtes détectés - affichage limité\n",
    "    print(\"\\nTypes mixtes détectés (object dtype)\")\n",
    "    print(f\" Dtype : {X_train_processed.dtype}\")\n",
    "    print(f\" Shape : {X_train_processed.shape}\")\n",
    "    print(\"\\nNote : Les statistiques de standardisation ne peuvent pas être calculées\")\n",
    "    print(\" avec des types mixtes, mais les données sont prêtes pour la modélisation.\")\n",
    "    print(\"\\nLes modèles comme LightGBM gèrent automatiquement les types mixtes.\")\n",
    "\n",
    "print(\"\\nStandardisation : OK (données preprocessées prêtes pour la modélisation)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sauvegarde des artifacts\n",
    "\n",
    "**Structure des fichiers créés :**\n",
    "\n",
    "```\n",
    "artifacts/\n",
    "|\n",
    "+-- preprocesseur.joblib\n",
    "| |\n",
    "| +-- ColumnTransformer fitté\n",
    "| +-- SimpleImputer (médianes calculées sur train)\n",
    "| +-- StandardScaler (mean=0, std=1 sur train)\n",
    "| +-- Taille : ~10 Ko\n",
    "| +-- Usage : preprocesseur.transform(nouvelles_données)\n",
    "|\n",
    "+-- data_split.joblib\n",
    "| |\n",
    "| +-- (X_train, X_valid, y_train, y_valid)\n",
    "| +-- X_train : (246 008, 795) - AVANT transform\n",
    "| +-- X_valid : (61 503, 795) - AVANT transform\n",
    "| +-- y_train : (246 008,)\n",
    "| +-- y_valid : (61 503,)\n",
    "| +-- Taille : ~300-400 Mo\n",
    "| +-- Usage : charger pour modélisation\n",
    "|\n",
    "+-- feature_names.joblib\n",
    "|\n",
    "+-- Liste des 795 noms de colonnes\n",
    "+-- Exemples : ['AMT_INCOME_TOTAL', 'BURO_DAYS_CREDIT_MEAN', ...]\n",
    "+-- Taille : ~20 Ko\n",
    "+-- Usage : interprétabilité (SHAP, feature importance)\n",
    "```\n",
    "\n",
    "**3 fichiers seront créés dans le dossier `artifacts/` :**\n",
    "\n",
    "1. **preprocesseur.joblib**\n",
    "- Contient le ColumnTransformer fitté\n",
    "- Peut être rechargé pour preprocessing des nouvelles données\n",
    "- Taille : quelques Ko\n",
    "\n",
    "2. **data_split.joblib**\n",
    "- Contient (X_train, X_valid, y_train, y_valid)\n",
    "- Les données sont AVANT transform (permet de refaire le preprocessing)\n",
    "- Taille : plusieurs centaines de Mo\n",
    "\n",
    "3. **feature_names.joblib**\n",
    "- Liste des noms de features\n",
    "- Utile pour l'interprétabilité (SHAP, feature importance)\n",
    "- Taille : quelques Ko\n",
    "\n",
    "**Ces fichiers seront utilisés dans les Notebooks 03, 04, 05...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du preprocesseur\n",
    "print(\"\\nSauvegarde des artifacts...\")\n",
    "\n",
    "# 1. Preprocesseur\n",
    "preprocesseur_path = DOSSIER_ARTIFACTS / 'preprocesseur.joblib'\n",
    "joblib.dump(preprocesseur, preprocesseur_path)\n",
    "print(f\"\\n1. Preprocesseur sauvegardé : {preprocesseur_path}\")\n",
    "print(f\" Taille : {preprocesseur_path.stat().st_size / 1024:.1f} Ko\")\n",
    "\n",
    "# 2. Data split (avec données preprocessées)\n",
    "# ATTENTION : On sauvegarde les données AVANT transform (X_train, X_valid)\n",
    "# pour permettre de refaire le transform si besoin dans les notebooks suivants\n",
    "data_split_path = DOSSIER_ARTIFACTS / 'data_split.joblib'\n",
    "joblib.dump((X_train, X_valid, y_train, y_valid), data_split_path)\n",
    "print(f\"\\n2. Data split sauvegardé : {data_split_path}\")\n",
    "print(f\" Taille : {data_split_path.stat().st_size / (1024**2):.1f} Mo\")\n",
    "print(f\" Contient : X_train, X_valid, y_train, y_valid (avant transform)\")\n",
    "\n",
    "# 3. Feature names\n",
    "feature_names = X_train.columns.tolist()\n",
    "feature_names_path = DOSSIER_ARTIFACTS / 'feature_names.joblib'\n",
    "joblib.dump(feature_names, feature_names_path)\n",
    "print(f\"\\n3. Feature names sauvegardé : {feature_names_path}\")\n",
    "print(f\" Taille : {feature_names_path.stat().st_size / 1024:.1f} Ko\")\n",
    "print(f\" Nombre de features : {len(feature_names)}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"SAUVEGARDE TERMINÉE AVEC SUCCÈS\")\n",
    "print(f\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Génération des Artifacts pour le Notebook 03\n",
    "\n",
    "Le Notebook 03 nécessite les données **transformées** (après application du preprocessing).\n",
    "\n",
    "Cette section génère les fichiers suivants dans `artifacts/` :\n",
    "\n",
    "| Fichier | Description | Taille approximative |\n",
    "|---------|-------------|---------------------|\n",
    "| `X_train_processed.joblib` | Features train transformées | ~1.5 Go |\n",
    "| `X_valid_processed.joblib` | Features valid transformées | ~380 Mo |\n",
    "| `y_train.joblib` | Target train | ~2 Mo |\n",
    "| `y_valid.joblib` | Target valid | ~500 Ko |\n",
    "\n",
    "**Note :** Cette étape peut prendre 1-2 minutes selon votre machine.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GÉNÉRATION DES ARTIFACTS POUR NOTEBOOK 03\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" GÉNÉRATION DES FICHIERS POUR NOTEBOOK 03\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nApplication du preprocessing...\")\n",
    "print(\"   (Cela peut prendre 1-2 minutes...)\\n\")\n",
    "\n",
    "# Application du preprocessing - Retourne des numpy arrays\n",
    "X_train_array = preprocesseur.transform(X_train)\n",
    "X_valid_array = preprocesseur.transform(X_valid)\n",
    "\n",
    "print(f\"   X_train transformé : {X_train_array.shape}\")\n",
    "print(f\"   X_valid transformé : {X_valid_array.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ÉTAPE CRITIQUE : CONVERSION EN DATAFRAME AVEC NOMS DE COLONNES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nConversion en DataFrames avec noms de colonnes...\")\n",
    "\n",
    "# Récupérer les noms de features après transformation\n",
    "try:\n",
    "    # Essayer d'obtenir les noms depuis le preprocesseur\n",
    "    feature_names_transformed = preprocesseur.get_feature_names_out()\n",
    "    print(f\"   Noms récupérés via get_feature_names_out()\")\n",
    "except AttributeError:\n",
    "    # Fallback : créer des noms descriptifs\n",
    "    print(f\"   get_feature_names_out() non disponible\")\n",
    "    print(f\"   Création de noms descriptifs...\")\n",
    "    \n",
    "    feature_names_base = X_train.columns.tolist()\n",
    "    feature_names_transformed = []\n",
    "    \n",
    "    for i in range(X_train_array.shape[1]):\n",
    "        if i < len(feature_names_base):\n",
    "            feature_names_transformed.append(f\"{feature_names_base[i]}_transformed_{i}\")\n",
    "        else:\n",
    "            feature_names_transformed.append(f\"feature_transformed_{i}\")\n",
    "    \n",
    "    print(f\"   {len(feature_names_transformed)} noms créés\")\n",
    "\n",
    "print(f\"\\n   Exemples de noms (5 premiers) :\")\n",
    "for i, name in enumerate(feature_names_transformed[:5], 1):\n",
    "    print(f\"      {i}. {name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# NETTOYAGE DES NOMS POUR LIGHTGBM\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nNettoyage des noms pour compatibilité LightGBM...\")\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_feature_name(name):\n",
    "    \"\"\"\n",
    "    Nettoie les noms de features pour LightGBM.\n",
    "    LightGBM rejette les caractères spéciaux JSON : \" [ ] : { } ,\n",
    "    \"\"\"\n",
    "    name = str(name)\n",
    "    # Remplacer caractères JSON par underscore\n",
    "    name = re.sub(r'[\"\\[\\]:{},]', '_', name)\n",
    "    # Garder seulement alphanumériques, underscores et tirets\n",
    "    name = re.sub(r'[^\\w\\-]', '_', name)\n",
    "    # Supprimer underscores multiples\n",
    "    name = re.sub(r'_+', '_', name)\n",
    "    # Supprimer underscores début/fin\n",
    "    name = name.strip('_')\n",
    "    return name\n",
    "\n",
    "# Nettoyer les noms\n",
    "feature_names_cleaned = [clean_feature_name(name) for name in feature_names_transformed]\n",
    "\n",
    "print(f\"   {len(feature_names_cleaned)} noms nettoyés\")\n",
    "print(f\"\\n   Exemples nettoyés (5 premiers) :\")\n",
    "for i, name in enumerate(feature_names_cleaned[:5], 1):\n",
    "    print(f\"      {i}. {name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CRÉATION DES DATAFRAMES AVEC NOMS DE COLONNES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nCréation des DataFrames avec colonnes nommées...\")\n",
    "\n",
    "# Convertir en DataFrame\n",
    "X_train_processed = pd.DataFrame(\n",
    "    X_train_array, \n",
    "    columns=feature_names_cleaned,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_valid_processed = pd.DataFrame(\n",
    "    X_valid_array, \n",
    "    columns=feature_names_cleaned,\n",
    "    index=X_valid.index\n",
    ")\n",
    "\n",
    "print(f\"   X_train_processed : {X_train_processed.shape}\")\n",
    "print(f\"      Type : {type(X_train_processed)}\")\n",
    "print(f\"      Colonnes : {X_train_processed.columns.tolist()[:3]}...\")\n",
    "\n",
    "print(f\"\\n   X_valid_processed : {X_valid_processed.shape}\")\n",
    "print(f\"      Type : {type(X_valid_processed)}\")\n",
    "print(f\"      Colonnes : {X_valid_processed.columns.tolist()[:3]}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAUVEGARDE DES FICHIERS AVEC NOMS DE COLONNES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nSauvegarde des fichiers transformés...\\n\")\n",
    "\n",
    "# X_train_processed (DataFrame avec noms)\n",
    "path = DOSSIER_ARTIFACTS / 'X_train_processed.joblib'\n",
    "joblib.dump(X_train_processed, path)\n",
    "print(f\"   X_train_processed.joblib : {path.stat().st_size / (1024**2):.1f} Mo\")\n",
    "\n",
    "# X_valid_processed (DataFrame avec noms)\n",
    "path = DOSSIER_ARTIFACTS / 'X_valid_processed.joblib'\n",
    "joblib.dump(X_valid_processed, path)\n",
    "print(f\"   X_valid_processed.joblib : {path.stat().st_size / (1024**2):.1f} Mo\")\n",
    "\n",
    "# y_train\n",
    "path = DOSSIER_ARTIFACTS / 'y_train.joblib'\n",
    "joblib.dump(y_train, path)\n",
    "print(f\"   y_train.joblib : {path.stat().st_size / 1024:.1f} Ko\")\n",
    "\n",
    "# y_valid\n",
    "path = DOSSIER_ARTIFACTS / 'y_valid.joblib'\n",
    "joblib.dump(y_valid, path)\n",
    "print(f\"   y_valid.joblib : {path.stat().st_size / 1024:.1f} Ko\")\n",
    "\n",
    "# ============================================================================\n",
    "# VÉRIFICATION FINALE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VÉRIFICATION FINALE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test de rechargement\n",
    "X_test_reload = joblib.load(DOSSIER_ARTIFACTS / 'X_train_processed.joblib')\n",
    "\n",
    "print(f\"\\nTest de rechargement X_train_processed.joblib :\")\n",
    "print(f\"   Type : {type(X_test_reload)}\")\n",
    "print(f\"   Shape : {X_test_reload.shape}\")\n",
    "\n",
    "if hasattr(X_test_reload, 'columns'):\n",
    "    print(f\"   A des colonnes nommées !\")\n",
    "    print(f\"\\n   10 premiers noms :\")\n",
    "    for i, col in enumerate(X_test_reload.columns[:10], 1):\n",
    "        print(f\"      {i:2d}. {col}\")\n",
    "else:\n",
    "    print(f\"   ERREUR : Pas de colonnes nommées !\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOUS LES ARTIFACTS GÉNÉRÉS AVEC SUCCÈS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nLe Notebook 03 peut maintenant être exécuté !\")\n",
    "print(\"Les DataFrames ont des noms de colonnes propres pour LightGBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récapitulatif\n",
    "\n",
    "### Ce qui a été fait dans ce notebook :\n",
    "\n",
    "1. **Chargement des données** : `application_train_AGGREGATED.csv` (307 511 × 797)\n",
    "\n",
    "2. **Séparation X et y** : Exclusion de SK_ID_CURR et TARGET\n",
    "\n",
    "3. **Split train/validation** : 80/20 avec stratification\n",
    "\n",
    "4. **Pipeline de preprocessing** :\n",
    "- Variables numériques : Imputation (médiane) + Standardisation\n",
    "- Variables catégorielles : Imputation ('missing')\n",
    "\n",
    "5. **Fit sur train, transform sur train et validation** (prévention data leakage)\n",
    "\n",
    "6. **Sauvegarde de 3 artifacts** pour réutilisation\n",
    "\n",
    "### Résultats :\n",
    "\n",
    "- **Train set** : 246 008 échantillons × 795 features\n",
    "- **Validation set** : 61 503 échantillons × 795 features\n",
    "- **Aucun NaN** après preprocessing\n",
    "- **Variables standardisées** (moyenne ≈ 0, écart-type ≈ 1)\n",
    "\n",
    "### Prochaines étapes :\n",
    "\n",
    "- **Notebook 03** : Entraînement et comparaison des modèles (Dummy, LogReg, LightGBM)\n",
    "- **Notebook 04** : Gestion du déséquilibre (SMOTE vs class_weight)\n",
    "- **Notebook 05** : Optimisation du seuil de décision métier\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script d'extraction des figures - Notebook 02\n",
    "Preprocessing et pipeline (traitement des données)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import base64\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_figures_from_notebook(notebook_path, output_dir):\n",
    "    \"\"\"\n",
    "    Extrait toutes les figures du notebook de preprocessing\n",
    "    \"\"\"\n",
    "    # Créer le dossier de sortie\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Charger le notebook\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = json.load(f)\n",
    "\n",
    "    figure_count = 0\n",
    "\n",
    "    # Parcourir toutes les cellules\n",
    "    for cell_idx, cell in enumerate(notebook['cells']):\n",
    "        # Chercher les cellules avec outputs\n",
    "        if cell['cell_type'] == 'code' and 'outputs' in cell:\n",
    "            for output in cell['outputs']:\n",
    "                # Chercher les images PNG\n",
    "                if 'data' in output and 'image/png' in output['data']:\n",
    "                    figure_count += 1\n",
    "\n",
    "                    # Décoder l'image base64\n",
    "                    img_data = output['data']['image/png']\n",
    "                    img_bytes = base64.b64decode(img_data)\n",
    "\n",
    "                    # Déterminer le nom du fichier selon le contexte\n",
    "                    cell_source = ''.join(cell['source']).lower()\n",
    "\n",
    "                    if 'pipeline' in cell_source or 'schema' in cell_source:\n",
    "                        filename = f'nb02_fig{figure_count:02d}_pipeline_schema.png'\n",
    "                    elif 'distribution' in cell_source and ('avant' in cell_source or 'après' in cell_source):\n",
    "                        filename = f'nb02_fig{figure_count:02d}_distributions_avant_apres.png'\n",
    "                    elif 'encoding' in cell_source or 'encodage' in cell_source:\n",
    "                        filename = f'nb02_fig{figure_count:02d}_encodage_categoriel.png'\n",
    "                    elif 'scaler' in cell_source or 'normalisation' in cell_source:\n",
    "                        filename = f'nb02_fig{figure_count:02d}_normalisation.png'\n",
    "                    elif 'outliers' in cell_source or 'aberrant' in cell_source:\n",
    "                        filename = f'nb02_fig{figure_count:02d}_outliers.png'\n",
    "                    else:\n",
    "                        filename = f'nb02_fig{figure_count:02d}_figure.png'\n",
    "\n",
    "                    # Sauvegarder l'image\n",
    "                    output_path = output_dir / filename\n",
    "                    with open(output_path, 'wb') as img_file:\n",
    "                        img_file.write(img_bytes)\n",
    "\n",
    "                    print(f\" Extrait : {filename}\")\n",
    "\n",
    "    print(f\"\\n Total : {figure_count} figures extraites du Notebook 02\")\n",
    "    return figure_count\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Chemins\n",
    "    NOTEBOOK_PATH = 'Barre_Stephane_P7_02_preparation_pipeline.ipynb'\n",
    "    OUTPUT_DIR = 'outputs/figures_p7/notebook_02'\n",
    "\n",
    "    # Extraction\n",
    "    print(\" Extraction des figures du Notebook 02...\")\n",
    "    print(f\" Notebook : {NOTEBOOK_PATH}\")\n",
    "    print(f\" Sortie : {OUTPUT_DIR}\\n\")\n",
    "\n",
    "    extract_figures_from_notebook(NOTEBOOK_PATH, OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

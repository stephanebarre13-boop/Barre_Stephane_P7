{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Notebook 06 : Interprétabilité avec SHAP\n",
    "\n",
    "**Projet 7 - Prêt à dépenser : Scoring Crédit**\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "1. Charger le pipeline et les données de validation\n",
    "2. Aligner les features avec le modèle\n",
    "3. Initialiser SHAP TreeExplainer\n",
    "4. Calculer les valeurs SHAP\n",
    "5. Créer des visualisations globales :\n",
    " - Summary plot (beeswarm)\n",
    " - Bar plot de l'importance\n",
    " - Dependence plots\n",
    "6. Créer des visualisations locales :\n",
    " - Waterfall plots pour des clients spécifiques\n",
    " - Force plots individuels\n",
    "7. Analyser l'impact des features principales\n",
    "8. Comparer des cas de crédit accordé vs refusé\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## 1. Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports standards\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# SHAP\n",
    "import shap\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('Set2')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Pour les notebooks\n",
    "shap.initjs()\n",
    "\n",
    "print(\" Imports réussis\")\n",
    "print(f\"Version SHAP: {shap.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "## 2. Chargement du Pipeline et des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins\n",
    "ARTIFACTS_DIR = Path('../artifacts')\n",
    "FIGURES_DIR = Path('../figures')\n",
    "FIGURES_DIR.mkdir(exist_ok=True) # Créer le dossier s'il n'existe pas\n",
    "\n",
    "PIPELINE_PATH = ARTIFACTS_DIR / 'pipeline_final.joblib'\n",
    "DATA_SPLIT_PATH = ARTIFACTS_DIR / 'data_split.joblib'\n",
    "FEATURE_NAMES_PATH = ARTIFACTS_DIR / 'feature_names.joblib'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CHARGEMENT DES DONNÉES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Charger le pipeline\n",
    "print(\"\\nChargement du pipeline...\")\n",
    "pipeline = joblib.load(PIPELINE_PATH)\n",
    "print(f\" Pipeline chargé: {type(pipeline)}\")\n",
    "\n",
    "# Charger les données de validation (on utilisera X_valid comme données de test)\n",
    "print(\"\\nChargement des données...\")\n",
    "X_train, X_valid, y_train, y_valid = joblib.load(DATA_SPLIT_PATH)\n",
    "print(f\" X_valid shape: {X_valid.shape}\")\n",
    "print(f\" y_valid shape: {y_valid.shape}\")\n",
    "\n",
    "# Charger les noms de features si disponibles\n",
    "try:\n",
    " feature_names = joblib.load(FEATURE_NAMES_PATH)\n",
    " print(f\"\\n {len(feature_names)} noms de features chargés\")\n",
    "except FileNotFoundError:\n",
    " feature_names = list(X_valid.columns)\n",
    " print(f\"\\n feature_names.joblib non trouvé, utilisation des colonnes de X_valid\")\n",
    " print(f\" {len(feature_names)} features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RÉSUMÉ\")\n",
    "print(\"=\"*60)\n",
    "print(f\" - Échantillon de validation: {X_valid.shape[0]:,} clients\")\n",
    "print(f\" - Features: {X_valid.shape[1]}\")\n",
    "print(f\" - Taux de défaut: {y_valid.mean():.2%}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "align_features",
   "metadata": {},
   "source": [
    "## 3. Alignement des Features avec le Modèle\n",
    "\n",
    "Important : Il faut s'assurer que les données ont le même nombre de colonnes que lors de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "align_features_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ALIGNEMENT DES FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Récupérer les features attendues par le modèle\n",
    "try:\n",
    " expected_features = pipeline.feature_name_\n",
    " print(\" Features récupérées via feature_name_\")\n",
    "except:\n",
    " try:\n",
    " expected_features = pipeline.booster_.feature_name()\n",
    " print(\" Features récupérées via booster_.feature_name()\")\n",
    " except:\n",
    " try:\n",
    " expected_features = pipeline._Booster.feature_name()\n",
    " print(\" Features récupérées via _Booster.feature_name()\")\n",
    " except:\n",
    " # Si impossible de récupérer, utiliser les feature_names chargés\n",
    " expected_features = feature_names\n",
    " print(\" Utilisation des feature_names chargés\")\n",
    "\n",
    "print(f\"\\nFeatures attendues par le modèle: {len(expected_features)}\")\n",
    "print(f\"Features dans X_valid: {len(X_valid.columns)}\")\n",
    "\n",
    "# Identifier les différences\n",
    "current_cols = set(X_valid.columns)\n",
    "expected_cols = set(expected_features)\n",
    "\n",
    "missing_cols = list(expected_cols - current_cols)\n",
    "extra_cols = list(current_cols - expected_cols)\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    " print(f\"\\n Colonnes manquantes: {len(missing_cols)}\")\n",
    " if len(missing_cols) <= 10:\n",
    " print(f\" {missing_cols}\")\n",
    " else:\n",
    " print(f\" {missing_cols[:5]}... (+{len(missing_cols)-5} autres)\")\n",
    " \n",
    " # Ajouter les colonnes manquantes\n",
    " print(\"\\nAjout des colonnes manquantes...\")\n",
    " missing_df = pd.DataFrame(0, index=X_valid.index, columns=missing_cols)\n",
    " X_valid = pd.concat([X_valid, missing_df], axis=1)\n",
    " print(\" Colonnes ajoutées\")\n",
    "\n",
    "if len(extra_cols) > 0:\n",
    " print(f\"\\n Colonnes en trop: {len(extra_cols)}\")\n",
    " if len(extra_cols) <= 10:\n",
    " print(f\" {extra_cols}\")\n",
    " else:\n",
    " print(f\" {extra_cols[:5]}... (+{len(extra_cols)-5} autres)\")\n",
    " \n",
    " # Supprimer les colonnes en trop\n",
    " print(\"\\nSuppression des colonnes en trop...\")\n",
    " X_valid = X_valid.drop(columns=extra_cols)\n",
    " print(\" Colonnes supprimées\")\n",
    "\n",
    "# Réordonner les colonnes\n",
    "print(\"\\nRéordonnancement des colonnes...\")\n",
    "X_valid = X_valid[expected_features]\n",
    "print(\" Colonnes réordonnées\")\n",
    "\n",
    "print(f\"\\n Shape finale: {X_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convert_types",
   "metadata": {},
   "source": [
    "## 4. Conversion des Types de Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convert_types_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CONVERSION DES TYPES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Identifier les colonnes object\n",
    "object_cols = X_valid.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"\\nColonnes 'object' à convertir: {len(object_cols)}\")\n",
    "\n",
    "# Convertir les colonnes object en numérique\n",
    "if len(object_cols) > 0:\n",
    " print(\"Conversion en cours...\")\n",
    " for col in object_cols:\n",
    " X_valid[col] = pd.to_numeric(X_valid[col], errors='coerce')\n",
    " print(\" Conversion terminée\")\n",
    "\n",
    "# Gérer les valeurs manquantes\n",
    "nan_count = X_valid.isna().sum().sum()\n",
    "if nan_count > 0:\n",
    " print(f\"\\nValeurs NaN détectées: {nan_count:,}\")\n",
    " X_valid = X_valid.fillna(0)\n",
    " print(\" NaN remplacés par 0\")\n",
    "\n",
    "print(f\"\\n Données prêtes pour SHAP\")\n",
    "print(f\" Shape: {X_valid.shape}\")\n",
    "print(f\" NaN restants: {X_valid.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shap_prep",
   "metadata": {},
   "source": [
    "## 5. Préparation de l'Échantillon pour SHAP\n",
    "\n",
    "Pour des raisons de performance, on utilise un échantillon des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap_prep_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PRÉPARATION DE L'ÉCHANTILLON SHAP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Définir la taille de l'échantillon\n",
    "# Pour SHAP, on recommande 100-500 observations pour un bon compromis vitesse/qualité\n",
    "sample_size = min(200, len(X_valid))\n",
    "\n",
    "# Échantillonnage stratifié pour garder la distribution des classes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if len(X_valid) > sample_size:\n",
    " X_sample, _, y_sample, _ = train_test_split(\n",
    " X_valid, y_valid, \n",
    " train_size=sample_size,\n",
    " stratify=y_valid,\n",
    " random_state=42\n",
    " )\n",
    " print(f\" Échantillon stratifié créé: {sample_size} observations\")\n",
    "else:\n",
    " X_sample = X_valid\n",
    " y_sample = y_valid\n",
    " print(f\" Utilisation de toutes les données: {len(X_sample)} observations\")\n",
    "\n",
    "print(f\"\\nDistribution dans l'échantillon:\")\n",
    "print(f\" Classe 0 (bon): {(y_sample == 0).sum()} ({(y_sample == 0).mean():.1%})\")\n",
    "print(f\" Classe 1 (défaut): {(y_sample == 1).sum()} ({(y_sample == 1).mean():.1%})\")\n",
    "\n",
    "# Générer les prédictions pour l'échantillon\n",
    "print(\"\\nGénération des prédictions...\")\n",
    "y_pred_proba = pipeline.predict_proba(X_sample)[:, 1]\n",
    "print(f\" Prédictions générées\")\n",
    "print(f\" Probabilité moyenne: {y_pred_proba.mean():.3f}\")\n",
    "print(f\" Min: {y_pred_proba.min():.3f}, Max: {y_pred_proba.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init_shap",
   "metadata": {},
   "source": [
    "## 6. Initialisation de SHAP TreeExplainer\n",
    "\n",
    "Pour les modèles LightGBM, on utilise **TreeExplainer** qui est optimisé et rapide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_shap_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"INITIALISATION DE SHAP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialiser l'explainer avec le modèle\n",
    "print(\"\\nInitialisation du SHAP TreeExplainer...\")\n",
    "explainer = shap.TreeExplainer(pipeline)\n",
    "print(f\" Explainer initialisé pour {type(pipeline).__name__}\")\n",
    "\n",
    "# Calculer les valeurs SHAP\n",
    "print(f\"\\nCalcul des valeurs SHAP pour {len(X_sample)} observations...\")\n",
    "print(\" Cela peut prendre quelques secondes...\")\n",
    "\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# Pour classification binaire, shap_values peut être:\n",
    "# - Une liste [class_0, class_1] pour certains modèles\n",
    "# - Un array unique pour d'autres\n",
    "if isinstance(shap_values, list):\n",
    " # Prendre les valeurs pour la classe positive (défaut = 1)\n",
    " shap_values_class1 = shap_values[1]\n",
    " print(\" Valeurs SHAP calculées (liste - pris classe 1)\")\n",
    "else:\n",
    " shap_values_class1 = shap_values\n",
    " print(\" Valeurs SHAP calculées (array unique)\")\n",
    "\n",
    "print(f\"\\nShape des valeurs SHAP: {shap_values_class1.shape}\")\n",
    "print(f\"Expected value (base): {explainer.expected_value if not isinstance(explainer.expected_value, list) else explainer.expected_value[1]:.4f}\")\n",
    "\n",
    "# Stocker la base value\n",
    "if isinstance(explainer.expected_value, list):\n",
    " base_value = explainer.expected_value[1]\n",
    "else:\n",
    " base_value = explainer.expected_value\n",
    "\n",
    "print(f\"\\n La base value ({base_value:.4f}) représente la prédiction moyenne du modèle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global_viz",
   "metadata": {},
   "source": [
    "## 7. Visualisations Globales\n",
    "\n",
    "### 7.1 Summary Plot (Beeswarm)\n",
    "\n",
    "Ce graphique montre :\n",
    "- **Axe vertical** : Features ordonnées par importance\n",
    "- **Axe horizontal** : Impact sur la prédiction\n",
    "- **Couleur** : Valeur de la feature (rouge = élevé, bleu = faible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SUMMARY PLOT (BEESWARM)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "shap.summary_plot(\n",
    " shap_values_class1,\n",
    " X_sample,\n",
    " max_display=20,\n",
    " show=False\n",
    ")\n",
    "plt.title(\"Impact des Features sur la Probabilité de Défaut\\n(SHAP Summary Plot)\", \n",
    " fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel(\"Impact SHAP sur la prédiction\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'shap_summary_beeswarm.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Summary plot créé et sauvegardé\")\n",
    "print(f\" Fichier: {FIGURES_DIR / 'shap_summary_beeswarm.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bar_plot",
   "metadata": {},
   "source": [
    "### 7.2 Bar Plot (Importance Moyenne Absolue)\n",
    "\n",
    "Ce graphique montre l'importance moyenne absolue de chaque feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bar_plot_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BAR PLOT (IMPORTANCE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "shap.summary_plot(\n",
    " shap_values_class1,\n",
    " X_sample,\n",
    " max_display=20,\n",
    " plot_type='bar',\n",
    " show=False\n",
    ")\n",
    "plt.title(\"Importance Moyenne des Features\\n(SHAP Bar Plot)\", \n",
    " fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel(\"Importance moyenne (|SHAP value|)\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'shap_bar_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Bar plot créé et sauvegardé\")\n",
    "print(f\" Fichier: {FIGURES_DIR / 'shap_bar_plot.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_importance",
   "metadata": {},
   "source": [
    "### 7.3 Top Features\n",
    "\n",
    "Calculons les features les plus importantes selon SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_importance_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"IMPORTANCE DES FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculer l'importance moyenne absolue\n",
    "feature_importance_shap = pd.DataFrame({\n",
    " 'feature': X_sample.columns,\n",
    " 'importance': np.abs(shap_values_class1).mean(axis=0)\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Features les plus importantes (SHAP):\")\n",
    "print(\"=\"*60)\n",
    "for idx, row in feature_importance_shap.head(15).iterrows():\n",
    " print(f\"{row['feature']:50s} : {row['importance']:.6f}\")\n",
    "\n",
    "# Sauvegarder\n",
    "feature_importance_shap.to_csv(ARTIFACTS_DIR / 'shap_feature_importance.csv', index=False)\n",
    "print(f\"\\n Importance SHAP sauvegardée\")\n",
    "print(f\" Fichier: {ARTIFACTS_DIR / 'shap_feature_importance.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependence",
   "metadata": {},
   "source": [
    "### 7.4 Dependence Plots\n",
    "\n",
    "Ces graphiques montrent comment la valeur d'une feature influence la prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependence_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DEPENDENCE PLOTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prendre les 6 features les plus importantes\n",
    "top_features = feature_importance_shap.head(6)['feature'].tolist()\n",
    "\n",
    "print(f\"\\nCréation des dependence plots pour les {len(top_features)} features les plus importantes...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature_name in enumerate(top_features):\n",
    " # Trouver l'index de la feature\n",
    " feature_idx = list(X_sample.columns).index(feature_name)\n",
    " \n",
    " plt.sca(axes[idx])\n",
    " shap.dependence_plot(\n",
    " feature_idx,\n",
    " shap_values_class1,\n",
    " X_sample,\n",
    " show=False,\n",
    " ax=axes[idx]\n",
    " )\n",
    " axes[idx].set_title(f\"Dependence: {feature_name}\", fontsize=13, fontweight='bold')\n",
    " axes[idx].set_xlabel(feature_name, fontsize=11)\n",
    " axes[idx].set_ylabel(\"Impact SHAP\", fontsize=11)\n",
    "\n",
    "plt.suptitle(\"Dependence Plots - Top 6 Features\", fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'shap_dependence_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Dependence plots créés et sauvegardés\")\n",
    "print(f\" Fichier: {FIGURES_DIR / 'shap_dependence_plots.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local_viz",
   "metadata": {},
   "source": [
    "## 8. Visualisations Locales (Clients Individuels)\n",
    "\n",
    "### 8.1 Sélection d'Exemples Intéressants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select_examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SÉLECTION D'EXEMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Trouver des exemples intéressants:\n",
    "# 1. Client avec très faible risque (crédit accordé)\n",
    "idx_low_risk = np.argmin(y_pred_proba)\n",
    "\n",
    "# 2. Client avec risque élevé (crédit refusé)\n",
    "idx_high_risk = np.argmax(y_pred_proba)\n",
    "\n",
    "# 3. Client proche du seuil de décision\n",
    "# Charger le seuil optimal si disponible\n",
    "try:\n",
    " params = joblib.load(ARTIFACTS_DIR / 'parametres_decision.joblib')\n",
    " seuil = params['seuil_optimal']\n",
    " print(f\" Seuil optimal chargé: {seuil:.4f}\")\n",
    "except:\n",
    " seuil = 0.5\n",
    " print(f\" Utilisation du seuil par défaut: {seuil}\")\n",
    "\n",
    "idx_borderline = np.argmin(np.abs(y_pred_proba - seuil))\n",
    "\n",
    "# Afficher les exemples sélectionnés\n",
    "print(\"\\nExemples sélectionnés:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "exemples = [\n",
    " (\" FAIBLE RISQUE\", idx_low_risk),\n",
    " (\" BORDERLINE\", idx_borderline),\n",
    " (\" RISQUE ÉLEVÉ\", idx_high_risk)\n",
    "]\n",
    "\n",
    "for nom, idx in exemples:\n",
    " proba = y_pred_proba[idx]\n",
    " vrai_label = \"Défaut\" if y_sample.iloc[idx] == 1 else \"Bon\"\n",
    " decision = \"REFUSÉ\" if proba >= seuil else \"ACCEPTÉ\"\n",
    " \n",
    " print(f\"\\n{nom} (Index: {idx})\")\n",
    " print(f\" Probabilité de défaut: {proba:.4f}\")\n",
    " print(f\" Vraie classe: {vrai_label}\")\n",
    " print(f\" Décision: {decision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waterfall",
   "metadata": {},
   "source": [
    "### 8.2 Waterfall Plots\n",
    "\n",
    "Les waterfall plots montrent comment chaque feature contribue à la prédiction pour un client spécifique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waterfall_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"WATERFALL PLOTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Créer un waterfall plot pour chaque exemple\n",
    "for nom, idx in exemples:\n",
    " print(f\"\\nCréation du waterfall plot pour: {nom}\")\n",
    " \n",
    " plt.figure(figsize=(12, 8))\n",
    " \n",
    " # Créer l'explication\n",
    " explanation = shap.Explanation(\n",
    " values=shap_values_class1[idx],\n",
    " base_values=base_value,\n",
    " data=X_sample.iloc[idx].values,\n",
    " feature_names=X_sample.columns.tolist()\n",
    " )\n",
    " \n",
    " shap.waterfall_plot(explanation, max_display=15, show=False)\n",
    " \n",
    " plt.title(f\"Waterfall Plot - {nom}\\nProbabilité: {y_pred_proba[idx]:.4f}\", \n",
    " fontsize=14, fontweight='bold', pad=20)\n",
    " plt.tight_layout()\n",
    " \n",
    " # Sauvegarder\n",
    " filename = f\"shap_waterfall_{nom.split()[1].lower()}.png\"\n",
    " plt.savefig(FIGURES_DIR / filename, dpi=300, bbox_inches='tight')\n",
    " plt.show()\n",
    " \n",
    " print(f\" Sauvegardé: {filename}\")\n",
    "\n",
    "print(\"\\n Tous les waterfall plots créés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "force_plot",
   "metadata": {},
   "source": [
    "### 8.3 Force Plots\n",
    "\n",
    "Les force plots montrent visuellement comment les features poussent la prédiction vers le haut ou vers le bas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "force_plot_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FORCE PLOTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Force plot pour chaque exemple\n",
    "for nom, idx in exemples:\n",
    " print(f\"\\nCréation du force plot pour: {nom}\")\n",
    " \n",
    " # Force plot\n",
    " shap.force_plot(\n",
    " base_value,\n",
    " shap_values_class1[idx],\n",
    " X_sample.iloc[idx],\n",
    " matplotlib=True,\n",
    " show=False,\n",
    " figsize=(20, 3)\n",
    " )\n",
    " \n",
    " plt.title(f\"Force Plot - {nom} (Proba: {y_pred_proba[idx]:.4f})\", \n",
    " fontsize=12, fontweight='bold')\n",
    " plt.tight_layout()\n",
    " \n",
    " # Sauvegarder\n",
    " filename = f\"shap_force_{nom.split()[1].lower()}.png\"\n",
    " plt.savefig(FIGURES_DIR / filename, dpi=300, bbox_inches='tight')\n",
    " plt.show()\n",
    " \n",
    " print(f\" Sauvegardé: {filename}\")\n",
    "\n",
    "print(\"\\n Tous les force plots créés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis",
   "metadata": {},
   "source": [
    "## 9. Analyse Comparative\n",
    "\n",
    "Comparons les features qui influencent les décisions pour les clients acceptés vs refusés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ANALYSE COMPARATIVE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Séparer les clients selon la décision\n",
    "accepted = y_pred_proba < seuil\n",
    "refused = y_pred_proba >= seuil\n",
    "\n",
    "print(f\"\\nDistribution des décisions:\")\n",
    "print(f\" Acceptés: {accepted.sum()} ({accepted.mean():.1%})\")\n",
    "print(f\" Refusés: {refused.sum()} ({refused.mean():.1%})\")\n",
    "\n",
    "# Calculer l'importance moyenne pour chaque groupe\n",
    "shap_accepted = np.abs(shap_values_class1[accepted]).mean(axis=0)\n",
    "shap_refused = np.abs(shap_values_class1[refused]).mean(axis=0)\n",
    "\n",
    "# Créer un DataFrame comparatif\n",
    "comparison_df = pd.DataFrame({\n",
    " 'feature': X_sample.columns,\n",
    " 'importance_accepted': shap_accepted,\n",
    " 'importance_refused': shap_refused,\n",
    " 'difference': shap_refused - shap_accepted\n",
    "}).sort_values('difference', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 features plus importantes pour les REFUSÉS:\")\n",
    "print(\"=\"*60)\n",
    "for idx, row in comparison_df.head(10).iterrows():\n",
    " print(f\"{row['feature']:45s} : +{row['difference']:.6f}\")\n",
    "\n",
    "print(\"\\nTop 10 features plus importantes pour les ACCEPTÉS:\")\n",
    "print(\"=\"*60)\n",
    "for idx, row in comparison_df.tail(10).iterrows():\n",
    " print(f\"{row['feature']:45s} : {row['difference']:.6f}\")\n",
    "\n",
    "# Sauvegarder\n",
    "comparison_df.to_csv(ARTIFACTS_DIR / 'shap_comparison_accepted_refused.csv', index=False)\n",
    "print(f\"\\n Analyse comparative sauvegardée\")\n",
    "print(f\" Fichier: {ARTIFACTS_DIR / 'shap_comparison_accepted_refused.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 10. Résumé et Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"RÉSUMÉ DE L'ANALYSE SHAP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n VISUALISATIONS CRÉÉES:\")\n",
    "print(\" Summary plot (beeswarm)\")\n",
    "print(\" Bar plot (importance)\")\n",
    "print(\" Dependence plots (top 6 features)\")\n",
    "print(\" Waterfall plots (3 exemples)\")\n",
    "print(\" Force plots (3 exemples)\")\n",
    "\n",
    "print(\"\\n FICHIERS SAUVEGARDÉS:\")\n",
    "print(f\" Figures: {FIGURES_DIR}/\")\n",
    "print(f\" Importance SHAP: {ARTIFACTS_DIR / 'shap_feature_importance.csv'}\")\n",
    "print(f\" Comparaison: {ARTIFACTS_DIR / 'shap_comparison_accepted_refused.csv'}\")\n",
    "\n",
    "print(\"\\n TOP 5 FEATURES LES PLUS IMPORTANTES:\")\n",
    "for idx, row in feature_importance_shap.head(5).iterrows():\n",
    " print(f\" {idx+1}. {row['feature']}\")\n",
    "\n",
    "print(\"\\n INSIGHTS CLÉS:\")\n",
    "print(\" - Les valeurs SHAP permettent d'expliquer chaque prédiction\")\n",
    "print(\" - Les features importantes varient selon le profil (accepté/refusé)\")\n",
    "print(\" - Les visualisations peuvent être utilisées pour le dashboard\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSE SHAP TERMINÉE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d386792-eef9-4380-bfc1-c37732921f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script d'extraction des figures - Notebook 06\n",
    "SHAP Interprétabilité (waterfall, summary, force plots)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import base64\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_figures_from_notebook(notebook_path, output_dir):\n",
    " \"\"\"\n",
    " Extrait toutes les figures SHAP du notebook d'interprétabilité\n",
    " \"\"\"\n",
    " # Créer le dossier de sortie\n",
    " output_dir = Path(output_dir)\n",
    " output_dir.mkdir(parents=True, exist_ok=True)\n",
    " \n",
    " # Charger le notebook\n",
    " with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    " notebook = json.load(f)\n",
    " \n",
    " figure_count = 0\n",
    " \n",
    " # Parcourir toutes les cellules\n",
    " for cell_idx, cell in enumerate(notebook['cells']):\n",
    " # Chercher les cellules avec outputs\n",
    " if cell['cell_type'] == 'code' and 'outputs' in cell:\n",
    " for output in cell['outputs']:\n",
    " # Chercher les images PNG\n",
    " if 'data' in output and 'image/png' in output['data']:\n",
    " figure_count += 1\n",
    " \n",
    " # Décoder l'image base64\n",
    " img_data = output['data']['image/png']\n",
    " img_bytes = base64.b64decode(img_data)\n",
    " \n",
    " # Déterminer le nom du fichier selon le contexte\n",
    " cell_source = ''.join(cell['source']).lower()\n",
    " \n",
    " if 'waterfall' in cell_source:\n",
    " filename = f'nb06_fig{figure_count:02d}_shap_waterfall.png'\n",
    " elif 'summary' in cell_source and 'plot' in cell_source:\n",
    " filename = f'nb06_fig{figure_count:02d}_shap_summary.png'\n",
    " elif 'force' in cell_source:\n",
    " filename = f'nb06_fig{figure_count:02d}_shap_force.png'\n",
    " elif 'dependence' in cell_source or 'dépendance' in cell_source:\n",
    " filename = f'nb06_fig{figure_count:02d}_shap_dependence.png'\n",
    " elif 'bar' in cell_source or 'importance' in cell_source:\n",
    " filename = f'nb06_fig{figure_count:02d}_shap_importance.png'\n",
    " else:\n",
    " filename = f'nb06_fig{figure_count:02d}_shap_figure.png'\n",
    " \n",
    " # Sauvegarder l'image\n",
    " output_path = output_dir / filename\n",
    " with open(output_path, 'wb') as img_file:\n",
    " img_file.write(img_bytes)\n",
    " \n",
    " print(f\" Extrait : {filename}\")\n",
    " \n",
    " print(f\"\\n Total : {figure_count} figures extraites du Notebook 06\")\n",
    " return figure_count\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    " # Chemins\n",
    " NOTEBOOK_PATH = 'Barre_Stephane_P7_06_interpretabilite_shap.ipynb'\n",
    " OUTPUT_DIR = 'outputs/figures_p7/notebook_06'\n",
    " \n",
    " # Extraction\n",
    " print(\" Extraction des figures du Notebook 06 (SHAP)...\")\n",
    " print(f\" Notebook : {NOTEBOOK_PATH}\")\n",
    " print(f\" Sortie : {OUTPUT_DIR}\\n\")\n",
    " \n",
    " extract_figures_from_notebook(NOTEBOOK_PATH, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18e067-75af-4b6a-956b-0181ebf6b524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01 : Agrégation des données  \n",
    "## Home Credit Default Risk\n",
    "\n",
    "## Contexte du projet\n",
    "\n",
    "**Problématique métier :**  \n",
    "Prédire le risque de défaut de paiement pour les clients de Home Credit afin d'optimiser l'octroi de crédit et de minimiser les pertes financières.\n",
    "\n",
    "**Objectif de ce notebook :**  \n",
    "Enrichir la table principale `application_train.csv` en agrégeant l'historique comportemental des clients provenant de 6 tables secondaires.  \n",
    "Cette étape est cruciale car les données historiques sont fortement prédictives du comportement futur de remboursement.\n",
    "\n",
    "---\n",
    "\n",
    "## Schéma de la base de données\n",
    "\n",
    "![Schéma des tables](./DB.png)\n",
    "\n",
    "**Description des tables :**\n",
    "\n",
    "1. **application_train.csv** : Table principale contenant les informations statiques sur chaque demande de crédit (307 511 clients)\n",
    "   - Note: Pour ce projet académique, nous utilisons SEULEMENT application_train.csv (pas de test.csv)  \n",
    "   - Informations démographiques (âge, genre, situation familiale)  \n",
    "   - Informations professionnelles (revenus, type d'emploi, ancienneté)  \n",
    "   - Détails du crédit demandé (montant, annuité, prix du bien)\n",
    "\n",
    "2. **bureau.csv** : Tous les crédits antérieurs du client auprès d'autres institutions financières rapportés au bureau de crédit (1 716 428 lignes)  \n",
    "   - Une ligne par crédit externe du client  \n",
    "   - Informations sur le montant, le statut (actif/fermé), les retards éventuels\n",
    "\n",
    "3. **bureau_balance.csv** : Soldes mensuels des crédits bureau (27 299 925 lignes)  \n",
    "   - Historique mois par mois du statut de chaque crédit bureau  \n",
    "   - Permet de détecter les patterns de retard de paiement\n",
    "\n",
    "4. **previous_application.csv** : Toutes les demandes de crédit antérieures du client chez Home Credit (1 670 214 lignes)  \n",
    "   - Une ligne par demande passée  \n",
    "   - Informations sur le statut (approuvée/refusée), les montants, les conditions\n",
    "\n",
    "5. **POS_CASH_balance.csv** : Soldes mensuels des prêts POS et Cash antérieurs chez Home Credit (10 001 358 lignes)  \n",
    "   - Historique mois par mois des crédits POS/Cash  \n",
    "   - Indicateurs de retard (DPD : Days Past Due)\n",
    "\n",
    "6. **installments_payments.csv** : Historique de remboursement des échéances pour les crédits Home Credit antérieurs (13 605 401 lignes)  \n",
    "   - Une ligne par échéance (prévue ou réalisée)  \n",
    "   - Permet de calculer précisément les retards et sous-paiements\n",
    "\n",
    "7. **credit_card_balance.csv** : Soldes mensuels des cartes de crédit antérieures chez Home Credit (3 840 312 lignes)  \n",
    "   - Historique mois par mois des cartes de crédit  \n",
    "   - Informations sur l'utilisation du crédit, les retraits, les retards\n",
    "\n",
    "**Total : environ 58 millions de lignes à agréger par client**\n",
    "\n",
    "---\n",
    "\n",
    "## Méthodologie d'agrégation\n",
    "\n",
    "**Approche générale :**  \n",
    "Pour chaque table secondaire, nous appliquons le processus suivant :\n",
    "\n",
    "1. **Groupby** par `SK_ID_CURR` (identifiant client unique)  \n",
    "2. **Agrégations statistiques** : min, max, mean, sum, var selon la nature de la variable  \n",
    "3. **Merge** avec la table `application_train` sur la clé `SK_ID_CURR`\n",
    "\n",
    "**Agrégations spécifiques :**\n",
    "\n",
    "- **Bureau** : Séparation des crédits actifs vs fermés  \n",
    "- **Previous** : Séparation des demandes approuvées vs refusées  \n",
    "- **Installments** : Calcul des indicateurs de retard (DPD) et paiement anticipé (DBD)\n",
    "\n",
    "**Feature engineering :**  \n",
    "En plus des agrégations simples, nous créons des ratios métier pertinents :\n",
    "\n",
    "- Ratio revenu/crédit (capacité de remboursement)  \n",
    "- Ratio annuité/revenu (taux d'endettement)  \n",
    "- Taux de retard de paiement  \n",
    "- Différence entre montant payé et attendu  \n",
    "\n",
    "---\n",
    "\n",
    "## Sources et Références\n",
    "\n",
    "Ce notebook s'appuie sur les meilleures pratiques de la communauté Kaggle pour le challenge Home Credit Default Risk.\n",
    "\n",
    "### Kernel Principal de Référence\n",
    "\n",
    "**jsaguiar - \"LightGBM with Simple Features\"**  \n",
    "- **URL :** https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features  \n",
    "- **Performance :** Top 2% du challenge (Score AUC : 0.792)  \n",
    "- **Votes :** 1000+ upvotes sur Kaggle  \n",
    "\n",
    "**Éléments réutilisés de ce kernel :**\n",
    "\n",
    "- Structure d'agrégation hiérarchique (bureau → previous → POS/CC)  \n",
    "- Fonction `one_hot_encoder()`  \n",
    "- Fonctions d'agrégation :  \n",
    "  `bureau_and_balance()`, `previous_applications()`, `pos_cash()`,  \n",
    "  `installments_payments()`, `credit_card_balance()`\n",
    "\n",
    "**Adaptations et améliorations apportées :**\n",
    "\n",
    "- Documentation complète : Docstrings détaillées en français  \n",
    "- Explications pédagogiques : Sections narratives expliquant le \"pourquoi\"  \n",
    "- Contexte métier : Impact des features  \n",
    "- Réorganisation : 24 cellules claires vs 48 originales  \n",
    "- Commentaires détaillés : Code commenté ligne par ligne  \n",
    "- Adaptation locale : Chemins et configurations adaptés aux données locales  \n",
    "\n",
    "### Autres Références\n",
    "\n",
    "**Will Koehrsen - \"Introduction to Manual Feature Engineering\"**  \n",
    "Utilisé pour les concepts de feature engineering et la création de ratios métier.\n",
    "\n",
    "---\n",
    "\n",
    "## Note sur la Réutilisation de Code\n",
    "\n",
    "Cette approche de réutilisation documentée est conforme aux bonnes pratiques académiques et professionnelles en Data Science :\n",
    "\n",
    "1. Étude des références : Analyse des kernels Kaggle les plus performants  \n",
    "2. Réutilisation transparente : Code adapté avec citation claire des sources originales  \n",
    "3. Valeur ajoutée : Documentation extensive, explications métier, adaptation au projet  \n",
    "4. Transparence totale : Toutes les sources sont clairement identifiées et créditées  \n",
    "\n",
    "Documentation complète des sources : voir `SOURCES_ET_REFERENCES.txt`.\n",
    "\n",
    "---\n",
    "\n",
    "## Résultat attendu\n",
    "\n",
    "- **Colonnes initiales** : 122 (table application seule)  \n",
    "- **Colonnes finales** : environ 250-280 (après agrégation des 7 tables)  \n",
    "- **Nouvelles features** : environ 130-160 variables comportementales  \n",
    "- **Durée d'exécution** : 30-45 minutes  \n",
    "- **Fichier de sortie** : `application_train_AGGREGATED.csv` (environ 250-300 MB)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports des librairies nécessaires\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc  # Garbage collector pour libérer la mémoire\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Masquer les avertissements non critiques\n",
    "\n",
    "print('Imports réussis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Différence avec le kernel Kaggle original :\n",
    "\n",
    "- **Version Kaggle (originale) :** Utilise `application_train.csv` ET `application_test.csv`\n",
    "  - But : Entraîner sur train, prédire sur test, soumettre à Kaggle\n",
    "  \n",
    "- **Version Openclassrooms (ce notebook) :** Utilise **SEULEMENT** `application_train.csv`\n",
    "  - But : Créer notre propre split train/validation dans le Notebook 02\n",
    "  - Permet d'évaluer le modèle avec des métriques fiables (on a le TARGET)\n",
    "\n",
    "### Modifications apportées :\n",
    "\n",
    "1.  Fonction `application_train_test()` renommée en `application_train_only()`\n",
    "2.  Suppression du chargement de `application_test.csv`\n",
    "3.  Suppression de la concaténation train+test\n",
    "4.  Le split train/validation sera fait dans le Notebook 02 avec `train_test_split()`\n",
    "\n",
    "**Résultat :** Dataset final avec 307,511 clients (au lieu de 356,255) prêt pour le split train/validation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration des chemins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Configuration des chemins d'accès aux fichiers\n",
    "#\n",
    "# Structure attendue du dossier data:\n",
    "# data/\n",
    "#   ├── application_train.csv\n",
    "#   ├── application_test.csv\n",
    "#   ├── bureau.csv\n",
    "#   ├── bureau_balance.csv\n",
    "#   ├── previous_application.csv\n",
    "#   ├── POS_CASH_balance.csv\n",
    "#   ├── installments_payments.csv\n",
    "#   └── credit_card_balance.csv\n",
    "\n",
    "# Detecter automatiquement le bon chemin\n",
    "current_dir = Path.cwd()\n",
    "print(f\"Repertoire courant: {current_dir}\")\n",
    "\n",
    "# Chercher le dossier data\n",
    "if (current_dir / 'data').exists():\n",
    "    DATA_DIR = current_dir / 'data'\n",
    "elif (current_dir.parent / 'data').exists():\n",
    "    DATA_DIR = current_dir.parent / 'data'\n",
    "elif (current_dir / 'PROJET_P7_PREMIUM_AVEC_AGREGATION' / 'data').exists():\n",
    "    DATA_DIR = current_dir / 'PROJET_P7_PREMIUM_AVEC_AGREGATION' / 'data'\n",
    "else:\n",
    "    # Lister les dossiers disponibles\n",
    "    print(\"\\nDossiers disponibles dans le repertoire courant:\")\n",
    "    for item in current_dir.iterdir():\n",
    "        if item.is_dir():\n",
    "            print(f\"  - {item.name}\")\n",
    "    \n",
    "    # Chercher data dans les sous-dossiers\n",
    "    data_folders = list(current_dir.glob('**/data'))\n",
    "    if data_folders:\n",
    "        DATA_DIR = data_folders[0]\n",
    "        print(f\"\\nDossier 'data' trouve: {DATA_DIR}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Impossible de trouver le dossier 'data'. Verifiez votre emplacement.\")\n",
    "\n",
    "print(f\"Dossier data utilise: {DATA_DIR}\\n\")\n",
    "\n",
    "FILES = {\n",
    "    'train': DATA_DIR / 'application_train.csv',\n",
    "    # 'test': DATA_DIR / 'application_test.csv',  # Non utilise pour projet academique\n",
    "    'bureau': DATA_DIR / 'bureau.csv',\n",
    "    'bureau_balance': DATA_DIR / 'bureau_balance.csv',\n",
    "    'previous': DATA_DIR / 'previous_application.csv',\n",
    "    'pos': DATA_DIR / 'POS_CASH_balance.csv',\n",
    "    'installments': DATA_DIR / 'installments_payments.csv',\n",
    "    'credit_card': DATA_DIR / 'credit_card_balance.csv'\n",
    "}\n",
    "\n",
    "# Verification de la presence des fichiers\n",
    "print('Verification des fichiers de donnees:\\n')\n",
    "all_present = True\n",
    "for name, path in FILES.items():\n",
    "    exists = path.exists()\n",
    "    status = '[OK]' if exists else '[MANQUANT]'\n",
    "    print(f'{status:12} {name:15} : {path.name}')\n",
    "    if not exists:\n",
    "        all_present = False\n",
    "\n",
    "if not all_present:\n",
    "    print(f\"\\nERREUR: Certains fichiers sont manquants dans {DATA_DIR}\")\n",
    "    print(\"\\nFichiers CSV presents dans le dossier data:\")\n",
    "    csv_files = list(DATA_DIR.glob('*.csv'))\n",
    "    if csv_files:\n",
    "        for f in sorted(csv_files):\n",
    "            print(f\"  - {f.name}\")\n",
    "    else:\n",
    "        print(\"  (aucun fichier CSV trouve)\")\n",
    "else:\n",
    "    print(\"\\nTous les fichiers requis sont presents !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction utilitaire : Encodage des variables catégorielles\n",
    "\n",
    "**Problématique :**  \n",
    "Les algorithmes de machine learning (comme LightGBM ou la régression logistique) ne peuvent pas traiter directement les variables catégorielles sous forme de texte. Il faut les transformer en variables numériques.\n",
    "\n",
    "**Solution : One-Hot Encoding**\n",
    "\n",
    "Cette technique crée une colonne binaire (0/1) pour chaque modalité de la variable catégorielle.\n",
    "\n",
    "**Exemple :**\n",
    "\n",
    "Avant One-Hot Encoding:  \n",
    "CODE_GENDER  \n",
    "\n",
    "M  \n",
    "F  \n",
    "M  \n",
    "\n",
    "Après One-Hot Encoding:  \n",
    "CODE_GENDER_M  CODE_GENDER_F  \n",
    "\n",
    "1              0  \n",
    "0              1  \n",
    "1              0  \n",
    "\n",
    "**Paramètre `nan_as_category` :**\n",
    "- Si `True` : crée une colonne supplémentaire pour les valeurs manquantes  \n",
    "- Si `False` : les NaN restent NaN après l'encodage  \n",
    "\n",
    "Cette fonction sera utilisée sur toutes les tables pour encoder les variables catégorielles avant agrégation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    \"\"\"\n",
    "    Encode les variables catégorielles en variables binaires (One-Hot Encoding)\n",
    "    \n",
    "    Source : jsaguiar - \"LightGBM with Simple Features\"\n",
    "    Adaptations : Documentation complète ajoutée, commentaires détaillés\n",
    "    \n",
    "    Paramètres:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        DataFrame contenant les données à encoder\n",
    "    nan_as_category : bool, default=True\n",
    "        Si True, crée une colonne pour les valeurs manquantes (NaN)\n",
    "    \n",
    "    Retourne:\n",
    "    ---------\n",
    "    df : DataFrame\n",
    "        DataFrame avec variables catégorielles encodées en binaires\n",
    "    new_columns : list\n",
    "        Liste des nouvelles colonnes créées par l'encoding\n",
    "    \"\"\"\n",
    "    \n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    \n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    \n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    \n",
    "    return df, new_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Traitement de la table APPLICATION et Feature Engineering\n",
    "\n",
    "### Objectif\n",
    "Préparer la table principale `application_train.csv` et créer des features métier pertinentes avant d'agréger les autres tables.\n",
    "\n",
    "### Étapes de traitement\n",
    "\n",
    "**1. Concaténation train + test**\n",
    "- Permet d'appliquer les mêmes transformations sur les deux datasets\n",
    "- La variable `TARGET` (0/1) n'existe que dans le train\n",
    "\n",
    "**2. Nettoyage des données**\n",
    "- Suppression des 4 lignes avec `CODE_GENDER = 'XNA'` (valeur aberrante)\n",
    "- Correction de `DAYS_EMPLOYED = 365243` (code pour 'inconnu') en `NaN`\n",
    "\n",
    "**3. Encodage des variables binaires**\n",
    "- `CODE_GENDER` : M/F encodé en 0/1\n",
    "- `FLAG_OWN_CAR` : Y/N encodé en 0/1\n",
    "- `FLAG_OWN_REALTY` : Y/N encodé en 0/1\n",
    "\n",
    "**4. One-Hot Encoding des autres catégorielles**\n",
    "- Exemple : `NAME_EDUCATION_TYPE` → 5 colonnes binaires (une par niveau d'éducation)\n",
    "\n",
    "**5. Feature Engineering : Ratios métier**\n",
    "\n",
    "Ces ratios capturent des relations importantes entre les variables :\n",
    "\n",
    "| Feature | Formule | Interprétation métier |\n",
    "|---------|---------|----------------------|\n",
    "| `DAYS_EMPLOYED_PERC` | `DAYS_EMPLOYED / DAYS_BIRTH` | Stabilité professionnelle : quelle part de sa vie il a travaillé |\n",
    "| `INCOME_CREDIT_PERC` | `AMT_INCOME_TOTAL / AMT_CREDIT` | Capacité de remboursement : revenu par rapport au crédit demandé |\n",
    "| `INCOME_PER_PERSON` | `AMT_INCOME_TOTAL / CNT_FAM_MEMBERS` | Revenu disponible par personne du foyer |\n",
    "| `ANNUITY_INCOME_PERC` | `AMT_ANNUITY / AMT_INCOME_TOTAL` | Taux d'endettement : part du revenu consacrée au crédit |\n",
    "| `PAYMENT_RATE` | `AMT_ANNUITY / AMT_CREDIT` | Taux de remboursement annuel |\n",
    "\n",
    "**Pourquoi ces ratios sont-ils importants ?**\n",
    "- Un ratio `INCOME_CREDIT_PERC` élevé indique une bonne capacité de remboursement\n",
    "- Un ratio `ANNUITY_INCOME_PERC` trop élevé (>40%) peut indiquer un sur-endettement\n",
    "- Ces features créées manuellement améliorent significativement les performances du modèle\n",
    "\n",
    "**Référence :** Ces features sont inspirées des kernels Kaggle gagnants (jsaguiar, tunguz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def application_train_only(num_rows=None, nan_as_category=False):\n",
    "    \"\"\"\n",
    "    Charge et prétraite SEULEMENT application_train (version académique)\n",
    "    \n",
    "    MODIFICATION: Version adaptée pour projet académique sans application_test.csv\n",
    "    Le code original (jsaguiar) utilisait train+test pour la compétition Kaggle.\n",
    "    \n",
    "    Source : jsaguiar - \"LightGBM with Simple Features\"\n",
    "    Adaptations : \n",
    "    - Suppression de l'utilisation de application_test.csv\n",
    "    - Documentation des features créées\n",
    "    - Ajout explications ratios métier\n",
    "    \n",
    "    Features créées (exemples) :\n",
    "    - DAYS_EMPLOYED_PERC : Ratio ancienneté emploi / âge\n",
    "    - INCOME_CREDIT_PERC : Ratio revenu / montant crédit (capacité remboursement)\n",
    "    - INCOME_PER_PERSON : Revenu par personne dans le foyer\n",
    "    - ANNUITY_INCOME_PERC : Taux d'endettement (annuité / revenu)\n",
    "    \n",
    "    Paramètres:\n",
    "    -----------\n",
    "    num_rows : int, optional\n",
    "        Nombre de lignes à charger (pour tests). None = tout charger\n",
    "    nan_as_category : bool, default=False\n",
    "        Traiter les NaN comme une catégorie lors de l'encoding\n",
    "    \n",
    "    Retourne:\n",
    "    ---------\n",
    "    df : DataFrame\n",
    "        DataFrame prétraité avec features engineered\n",
    "    \"\"\"\n",
    "    \n",
    "    print('\\n' + '='*60)\n",
    "    print('TRAITEMENT TABLE APPLICATION (TRAIN SEULEMENT)')\n",
    "    print('='*60)\n",
    "    \n",
    "    # Lecture SEULEMENT des données train\n",
    "    df = pd.read_csv(FILES['train'], nrows=num_rows)\n",
    "    print(f\"Échantillons train: {len(df)}\")\n",
    "    \n",
    "    # NOTE: Pas de chargement de test_df ni de concaténation\n",
    "    # Le split train/validation sera fait dans le notebook 02\n",
    "    \n",
    "    # Nettoyage : suppression des 4 lignes avec CODE_GENDER = 'XNA' (valeur aberrante)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    # Encodage binaire des variables à 2 modalités\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    \n",
    "    # One-Hot Encoding des variables catégorielles restantes\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    # Correction valeur aberrante : DAYS_EMPLOYED = 365243 signifie 'inconnu'\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "    \n",
    "    # ==================== FEATURE ENGINEERING ====================\n",
    "    # Création de ratios métier pertinents pour la prédiction\n",
    "    \n",
    "    # 1. Ratio ancienneté emploi / âge (%)\n",
    "    # Plus ce ratio est élevé, plus la personne a travaillé longtemps relativement à son âge\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    \n",
    "    # 2. Ratio revenu / montant crédit (capacité de remboursement)\n",
    "    # Plus ce ratio est élevé, plus le client a les moyens de rembourser\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    \n",
    "    # 3. Revenu par personne dans le foyer\n",
    "    # Permet d'estimer le niveau de vie réel\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    \n",
    "    # 4. Taux d'endettement (annuité / revenu)\n",
    "    # Indicateur clé : plus il est élevé, plus le risque est important\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    \n",
    "    # 5. Taux de paiement (annuité / crédit)\n",
    "    # Permet d'estimer la durée du crédit\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    \n",
    "    # Suppression de la colonne FLAG_DOCUMENT_3 (trop de valeurs manquantes)\n",
    "    df = df.drop(['FLAG_DOCUMENT_3'], axis=1, errors='ignore')\n",
    "    \n",
    "    print(f\"Shape finale : {df.shape}\")\n",
    "    print(f\"Features créées : DAYS_EMPLOYED_PERC, INCOME_CREDIT_PERC, INCOME_PER_PERSON, etc.\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bureau + Bureau_Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_and_balance(num_rows=None, nan_as_category=True):\n",
    "    \"\"\"\n",
    "    Agrège les données des tables bureau.csv et bureau_balance.csv\n",
    "    \n",
    "    Source : jsaguiar - \"LightGBM with Simple Features\"\n",
    "    Adaptations : Documentation complète, explications hiérarchie agrégation\n",
    "    \n",
    "    Agrégation hiérarchique en 2 étapes :\n",
    "    1. bureau_balance → bureau (agrégation par SK_ID_BUREAU, niveau crédit)\n",
    "    2. bureau → application (agrégation par SK_ID_CURR, niveau client)\n",
    "    \n",
    "    Paramètres:\n",
    "    -----------\n",
    "    num_rows : int, optional\n",
    "        Nombre de lignes à charger. None = tout charger\n",
    "    nan_as_category : bool, default=True\n",
    "        Traiter les NaN comme une catégorie\n",
    "    \n",
    "    Retourne:\n",
    "    ---------\n",
    "    bureau_agg : DataFrame\n",
    "        DataFrame agrégé au niveau client avec statistiques bureau\n",
    "    \"\"\"\n",
    "    \n",
    "    print('\\n' + '='*60)\n",
    "    print('BUREAU + BUREAU_BALANCE')\n",
    "    print('='*60)\n",
    "    \n",
    "    bureau = pd.read_csv(FILES['bureau'], nrows=num_rows)\n",
    "    bb = pd.read_csv(FILES['bureau_balance'], nrows=num_rows)\n",
    "    print(f'Bureau: {bureau.shape}')\n",
    "    print(f'Bureau_balance: {bb.shape}')\n",
    "    \n",
    "    # One-hot encoding\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    print('\\n Agrégation bureau_balance...')\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    \n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    \n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace=True)\n",
    "    \n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    print('\\n Agrégations numériques...')\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    \n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat:\n",
    "        cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    \n",
    "    # Bureau: Active credits\n",
    "    print('\\n ACTIVE credits...')\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    \n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Bureau: Closed credits\n",
    "    print(' CLOSED credits...')\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    \n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f'\\n Bureau aggregated: {bureau_agg.shape}')\n",
    "    return bureau_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Previous Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_applications(num_rows=None, nan_as_category=True):\n",
    "    \"\"\"\n",
    "    Agrège les données de previous_application.csv\n",
    "    \n",
    "    Source : jsaguiar - \"LightGBM with Simple Features\"\n",
    "    Adaptations : Documentation complète, explications logique métier\n",
    "    \n",
    "    Analyse les demandes de crédit antérieures chez Home Credit.\n",
    "    Séparation approved/refused pour capturer patterns différents.\n",
    "    \n",
    "    Paramètres:\n",
    "    -----------\n",
    "    num_rows : int, optional\n",
    "        Nombre de lignes à charger. None = tout charger\n",
    "    nan_as_category : bool, default=True\n",
    "        Traiter les NaN comme une catégorie\n",
    "    \n",
    "    Retourne:\n",
    "    ---------\n",
    "    prev_agg : DataFrame\n",
    "        DataFrame agrégé au niveau client\n",
    "    \"\"\"\n",
    "    \n",
    "    print('\\n' + '='*60)\n",
    "    print('PREVIOUS APPLICATIONS')\n",
    "    print('='*60)\n",
    "    \n",
    "    prev = pd.read_csv(FILES['previous'], nrows=num_rows)\n",
    "    print(f'Previous: {prev.shape}')\n",
    "    \n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n",
    "    \n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n",
    "    \n",
    "    # FEATURE ENGINEERING\n",
    "    print('\\n Feature Engineering:')\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    print('   APP_CREDIT_PERC (demandé / reçu)')\n",
    "    \n",
    "    # Previous applications numeric features\n",
    "    print('\\n Agrégations numériques...')\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    \n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    \n",
    "    # Previous Applications: Approved Applications\n",
    "    print('\\n APPROVED applications...')\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    # Previous Applications: Refused Applications\n",
    "    print(' REFUSED applications...')\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f'\\n Previous aggregated: {prev_agg.shape}')\n",
    "    return prev_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. POS_CASH Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_cash(num_rows=None, nan_as_category=True):\n",
    "    \"\"\"\n",
    "    Agrège les données de POS_CASH_balance.csv\n",
    "    \n",
    "    Source : jsaguiar - \"LightGBM with Simple Features\"\n",
    "    Adaptations : Documentation complète, explications DPD (Days Past Due)\n",
    "    \n",
    "    Analyse l'historique mensuel des crédits POS (point de vente) et Cash.\n",
    "    Indicateurs de retard (DPD) importants pour prédiction.\n",
    "    \n",
    "    Paramètres:\n",
    "    -----------\n",
    "    num_rows : int, optional\n",
    "        Nombre de lignes à charger. None = tout charger\n",
    "    nan_as_category : bool, default=True\n",
    "        Traiter les NaN comme une catégorie\n",
    "    \n",
    "    Retourne:\n",
    "    ---------\n",
    "    pos_agg : DataFrame\n",
    "        DataFrame agrégé au niveau client\n",
    "    \"\"\"\n",
    "    \n",
    "    print('\\n' + '='*60)\n",
    "    print('POS_CASH BALANCE')\n",
    "    print('='*60)\n",
    "    \n",
    "    pos = pd.read_csv(FILES['pos'], nrows=num_rows)\n",
    "    print(f'POS_CASH: {pos.shape}')\n",
    "    \n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n",
    "    \n",
    "    # Features\n",
    "    print('\\n Agrégations...')\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    \n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    \n",
    "    del pos\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f'\\n POS_CASH aggregated: {pos_agg.shape}')\n",
    "    return pos_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Installments Payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def installments_payments(num_rows=None, nan_as_category=True):\n",
    "    \"\"\"\n",
    "    Agrège les données de installments_payments.csv\n",
    "    \n",
    "    Source : jsaguiar - \"LightGBM with Simple Features\"\n",
    "    Adaptations : Documentation complète, explications calculs retards\n",
    "    \n",
    "    Analyse l'historique des paiements d'échéances.\n",
    "    Calculs de retards et sous-paiements = indicateurs forts de risque.\n",
    "    \n",
    "    Paramètres:\n",
    "    -----------\n",
    "    num_rows : int, optional\n",
    "        Nombre de lignes à charger. None = tout charger\n",
    "    nan_as_category : bool, default=True\n",
    "        Traiter les NaN comme une catégorie\n",
    "    \n",
    "    Retourne:\n",
    "    ---------\n",
    "    ins_agg : DataFrame\n",
    "        DataFrame agrégé au niveau client\n",
    "    \"\"\"\n",
    "    \n",
    "    print('\\n' + '='*60)\n",
    "    print('INSTALLMENTS PAYMENTS')\n",
    "    print('='*60)\n",
    "    \n",
    "    ins = pd.read_csv(FILES['installments'], nrows=num_rows)\n",
    "    print(f'Installments: {ins.shape}')\n",
    "    \n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n",
    "    \n",
    "    # FEATURE ENGINEERING - Payment behavior\n",
    "    print('\\n Feature Engineering:')\n",
    "    \n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    print('   PAYMENT_PERC (payé / prévu)')\n",
    "    \n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    print('   PAYMENT_DIFF (différence paiement)')\n",
    "    \n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    print('   DPD (Days Past Due - retards)')\n",
    "    print('   DBD (Days Before Due - paiements anticipés)')\n",
    "    \n",
    "    # Features: Perform aggregations\n",
    "    print('\\n Agrégations...')\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    \n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    \n",
    "    del ins\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f'\\n Installments aggregated: {ins_agg.shape}')\n",
    "    return ins_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Credit Card Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_card_balance(num_rows=None, nan_as_category=True):\n",
    "    \"\"\"\n",
    "    Agrège les données de credit_card_balance.csv\n",
    "    \n",
    "    Source : jsaguiar - \"LightGBM with Simple Features\"\n",
    "    Adaptations : Documentation complète, explications utilisation crédit\n",
    "    \n",
    "    Analyse l'historique mensuel des cartes de crédit.\n",
    "    Taux d'utilisation du crédit = indicateur comportemental important.\n",
    "    \n",
    "    Paramètres:\n",
    "    -----------\n",
    "    num_rows : int, optional\n",
    "        Nombre de lignes à charger. None = tout charger\n",
    "    nan_as_category : bool, default=True\n",
    "        Traiter les NaN comme une catégorie\n",
    "    \n",
    "    Retourne:\n",
    "    ---------\n",
    "    cc_agg : DataFrame\n",
    "        DataFrame agrégé au niveau client\n",
    "    \"\"\"\n",
    "    \n",
    "    print('\\n' + '='*60)\n",
    "    print('CREDIT CARD BALANCE')\n",
    "    print('='*60)\n",
    "    \n",
    "    cc = pd.read_csv(FILES['credit_card'], nrows=num_rows)\n",
    "    print(f'Credit card: {cc.shape}')\n",
    "    \n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n",
    "    \n",
    "    # General aggregations\n",
    "    print('\\n Agrégations...')\n",
    "    cc.drop(['SK_ID_PREV'], axis=1, inplace=True)\n",
    "    \n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    \n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    \n",
    "    del cc\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f'\\n Credit card aggregated: {cc_agg.shape}')\n",
    "    return cc_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PIPELINE COMPLET - Exécution\n",
    "\n",
    "### Gestion de la mémoire\n",
    "\n",
    "Pour éviter les erreurs **MemoryError** lors de l'agrégation de ~58 millions de lignes :\n",
    "\n",
    "1. **Libération mémoire systématique** : `gc.collect()` après chaque merge\n",
    "2. **Suppression des variables** : `del variable` pour libérer la RAM immédiatement\n",
    "3. **Mode debug disponible** : Changez `debug = True` pour tester avec 10,000 lignes\n",
    "\n",
    "**RAM nécessaire** : ~2-4 GB pour le dataset complet\n",
    "\n",
    "### Note sur train/test\n",
    "\n",
    "Contrairement au kernel Kaggle original, nous n'utilisons **QUE** `application_train.csv` (pas de test.csv).\n",
    "\n",
    "Le split train/validation sera fait dans le Notebook 02 avec `train_test_split()`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print(' DEBUT AGGREGATION COMPLETE - 7 TABLES')\n",
    "print('='*80)\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION DES CHEMINS\n",
    "# ============================================================\n",
    "# Detecter si on est dans notebooks/ et remonter au niveau parent\n",
    "current_dir = Path.cwd()\n",
    "print(f\"Repertoire courant: {current_dir}\")\n",
    "\n",
    "# Si on est dans notebooks/, remonter d'un niveau\n",
    "if current_dir.name == 'notebooks':\n",
    "    DOSSIER_DATA = current_dir.parent / 'data'\n",
    "    print(f\"Detection: execution depuis notebooks/, remontee au niveau parent\")\n",
    "else:\n",
    "    DOSSIER_DATA = current_dir / 'data'\n",
    "\n",
    "print(f\"Dossier data utilise: {DOSSIER_DATA.absolute()}\")\n",
    "\n",
    "# Verifier que le dossier existe\n",
    "if not DOSSIER_DATA.exists():\n",
    "    print(f\"\\nERREUR: Le dossier {DOSSIER_DATA.absolute()} n'existe pas !\")\n",
    "    print(f\"\\nDossiers disponibles dans {current_dir.parent if current_dir.name == 'notebooks' else current_dir}:\")\n",
    "    parent = current_dir.parent if current_dir.name == 'notebooks' else current_dir\n",
    "    for item in parent.iterdir():\n",
    "        if item.is_dir():\n",
    "            print(f\"   - {item.name}\")\n",
    "    raise FileNotFoundError(f\"Dossier data non trouve: {DOSSIER_DATA}\")\n",
    "\n",
    "# Lister les fichiers disponibles\n",
    "print(f\"\\nFichiers CSV disponibles:\")\n",
    "csv_files = list(DOSSIER_DATA.glob('*.csv'))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"ERREUR: Aucun fichier CSV trouve dans {DOSSIER_DATA.absolute()} !\")\n",
    "\n",
    "for f in sorted(csv_files):\n",
    "    print(f\"   - {f.name}\")\n",
    "\n",
    "# Verifier que application_train.csv existe\n",
    "chemin_train = DOSSIER_DATA / 'application_train.csv'\n",
    "if not chemin_train.exists():\n",
    "    print(f\"\\nERREUR: Le fichier {chemin_train.name} n'existe pas !\")\n",
    "    print(f\"   Chemin cherche: {chemin_train.absolute()}\")\n",
    "    print(f\"\\nSolutions:\")\n",
    "    print(f\"   1. Verifiez que les fichiers CSV sont bien dans le dossier 'data/'\")\n",
    "    print(f\"   2. Telechargez les donnees depuis Kaggle si necessaire\")\n",
    "    raise FileNotFoundError(f\"Fichier requis non trouve: {chemin_train}\")\n",
    "\n",
    "print(f\"\\nOK - Fichier application_train.csv trouve\")\n",
    "\n",
    "# ============================================================\n",
    "# AGGREGATION\n",
    "# ============================================================\n",
    "start_time = time.time()\n",
    "\n",
    "# Debug mode (pour tester avec petit echantillon)\n",
    "debug = False\n",
    "num_rows = 10000 if debug else None\n",
    "\n",
    "if debug:\n",
    "    print(f\"\\nMODE DEBUG: Chargement de {num_rows} lignes uniquement\")\n",
    "else:\n",
    "    print(f\"\\nMODE COMPLET: Chargement de toutes les donnees\")\n",
    "\n",
    "# 1. Application + Feature Engineering\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"1. TRAITEMENT TABLE APPLICATION\")\n",
    "print(f\"{'='*60}\")\n",
    "df = application_train_only(num_rows)\n",
    "print(f'OK - Shape apres application: {df.shape}')\n",
    "\n",
    "# 2. Bureau + Bureau_Balance\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"2. TRAITEMENT BUREAU + BUREAU_BALANCE\")\n",
    "print(f\"{'='*60}\")\n",
    "bureau = bureau_and_balance(num_rows)\n",
    "df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "del bureau\n",
    "gc.collect()\n",
    "print(f'OK - Shape apres bureau: {df.shape}')\n",
    "\n",
    "# 3. Previous Applications\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"3. TRAITEMENT PREVIOUS APPLICATIONS\")\n",
    "print(f\"{'='*60}\")\n",
    "prev = previous_applications(num_rows)\n",
    "df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "del prev\n",
    "gc.collect()\n",
    "print(f'OK - Shape apres previous: {df.shape}')\n",
    "\n",
    "# 4. POS_CASH\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"4. TRAITEMENT POS_CASH\")\n",
    "print(f\"{'='*60}\")\n",
    "pos = pos_cash(num_rows)\n",
    "df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "del pos\n",
    "gc.collect()\n",
    "print(f'OK - Shape apres POS_CASH: {df.shape}')\n",
    "\n",
    "# 5. Installments\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"5. TRAITEMENT INSTALLMENTS\")\n",
    "print(f\"{'='*60}\")\n",
    "ins = installments_payments(num_rows)\n",
    "df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "del ins\n",
    "gc.collect()\n",
    "print(f'OK - Shape apres installments: {df.shape}')\n",
    "\n",
    "# 6. Credit Card\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"6. TRAITEMENT CREDIT CARD\")\n",
    "print(f\"{'='*60}\")\n",
    "cc = credit_card_balance(num_rows)\n",
    "df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "del cc\n",
    "gc.collect()\n",
    "print(f'OK - Shape apres credit_card: {df.shape}')\n",
    "\n",
    "# Note: Pas de separation train/test car nous utilisons SEULEMENT application_train.csv\n",
    "# Le split train/validation sera fait dans le Notebook 02 avec train_test_split()\n",
    "train_df = df  # Toutes les donnees sont deja dans train\n",
    "    \n",
    "# Liberation memoire finale\n",
    "del df\n",
    "gc.collect()\n",
    "    \n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# ============================================================\n",
    "# RESUME FINAL\n",
    "# ============================================================\n",
    "print('\\n' + '='*80)\n",
    "print(' AGGREGATION TERMINEE AVEC SUCCES')\n",
    "print('='*80)\n",
    "print(f'\\nRESULTATS FINAUX:')\n",
    "print(f'   Dataset shape: {train_df.shape}')\n",
    "print(f'   Nombre de clients: {train_df.shape[0]:,}')\n",
    "print(f'   Colonnes initiales: 122')\n",
    "print(f'   Colonnes finales: {train_df.shape[1]}')\n",
    "print(f'   Nouvelles features: {train_df.shape[1] - 122}')\n",
    "print(f'\\nDuree totale: {elapsed/60:.1f} minutes')\n",
    "\n",
    "# Verifier qu'on a bien TARGET\n",
    "if 'TARGET' in train_df.columns:\n",
    "    print(f'\\nDistribution de TARGET:')\n",
    "    print(f'   Remboursement (0): {(train_df[\"TARGET\"]==0).sum():,} ({(train_df[\"TARGET\"]==0).mean():.1%})')\n",
    "    print(f'   Defaut (1): {(train_df[\"TARGET\"]==1).sum():,} ({(train_df[\"TARGET\"]==1).mean():.1%})')\n",
    "else:\n",
    "    print(f'\\nWARNING: Colonne TARGET non trouvee !')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "\n",
    "# ============================================================\n",
    "# SAUVEGARDE\n",
    "# ============================================================\n",
    "print(f'\\nSauvegarde du fichier agrege...')\n",
    "chemin_sortie = DOSSIER_DATA / 'application_train_AGGREGATED.csv'\n",
    "train_df.to_csv(chemin_sortie, index=False)\n",
    "print(f'OK - Fichier sauvegarde: {chemin_sortie}')\n",
    "print(f'Taille: {chemin_sortie.stat().st_size / 1e6:.1f} MB')\n",
    "\n",
    "print('\\nPROCESSUS TERMINE !')\n",
    "print(f'Vous pouvez maintenant executer le Notebook 02 avec {chemin_sortie.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le fichier agrégé\n",
    "OUTPUT_FILE = DATA_DIR / 'application_train_AGGREGATED.csv'\n",
    "\n",
    "print(f'\\n Sauvegarde en cours...')\n",
    "train_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "file_size_mb = OUTPUT_FILE.stat().st_size / (1024**2)\n",
    "print(f'\\n Fichier sauvegardé: {OUTPUT_FILE}')\n",
    "print(f' Taille: {file_size_mb:.1f} MB')\n",
    "print(f' Shape: {train_df.shape}')\n",
    "\n",
    "# Statistiques\n",
    "print(f'\\n STATISTIQUES:')\n",
    "print(f'  NaN total: {train_df.isnull().sum().sum():,}')\n",
    "print(f'  % NaN: {train_df.isnull().sum().sum() / (train_df.shape[0] * train_df.shape[1]) * 100:.2f}%')\n",
    "\n",
    "# Exemples de nouvelles features\n",
    "new_features = [c for c in train_df.columns if any(p in c for p in \n",
    "    ['BURO_', 'ACTIVE_', 'CLOSED_', 'PREV_', 'APPROVED_', 'REFUSED_', \n",
    "     'POS_', 'INSTAL_', 'CC_', 'PERC', 'RATE', 'RATIO', 'DPD', 'DBD'])]\n",
    "\n",
    "print(f'\\nFEATURES CRÉÉES : {len(new_features)} nouvelles variables')\n",
    "print(f'  Exemples : {new_features[:10]}')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('NOTEBOOK 01 TERMINÉ AVEC SUCCÈS')\n",
    "print('='*80)\n",
    "print('\\nPROCHAINES ÉTAPES :')\n",
    "print('  1. Notebook 02 : Construction du pipeline de prétraitement sklearn')\n",
    "print('  2. Notebook 03 : Comparaison des modèles (Dummy, LogReg, LightGBM)')\n",
    "print('  3. Notebook 04 : Gestion du déséquilibre (SMOTE vs Class Weight)')\n",
    "print('  4. Notebook 05 : Optimisation du seuil de décision métier')\n",
    "print('\\n' + '='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier qu'on a bien toutes les features attendues\n",
    "print('\\nVÉRIFICATION DES FEATURES PAR TABLE :')\n",
    "print('\\n1. Application (ratios métier) :')\n",
    "app_features = [c for c in train_df.columns if 'PERC' in c or 'RATE' in c and 'BURO' not in c and 'PREV' not in c]\n",
    "print(f'   {len(app_features)} features : {app_features[:5]}')\n",
    "\n",
    "print('\\n2. Bureau :')\n",
    "buro_features = [c for c in train_df.columns if c.startswith('BURO_')]\n",
    "print(f'   {len(buro_features)} features')\n",
    "\n",
    "print('\\n3. Bureau Active/Closed :')\n",
    "active_features = [c for c in train_df.columns if c.startswith('ACTIVE_') or c.startswith('CLOSED_')]\n",
    "print(f'   {len(active_features)} features')\n",
    "\n",
    "print('\\n4. Previous :')\n",
    "prev_features = [c for c in train_df.columns if c.startswith('PREV_')]\n",
    "print(f'   {len(prev_features)} features')\n",
    "\n",
    "print('\\n5. Previous Approved/Refused :')\n",
    "appr_features = [c for c in train_df.columns if c.startswith('APPROVED_') or c.startswith('REFUSED_')]\n",
    "print(f'   {len(appr_features)} features')\n",
    "\n",
    "print('\\n6. POS_CASH :')\n",
    "pos_features = [c for c in train_df.columns if c.startswith('POS_')]\n",
    "print(f'   {len(pos_features)} features')\n",
    "\n",
    "print('\\n7. Installments (avec DPD/DBD) :')\n",
    "inst_features = [c for c in train_df.columns if c.startswith('INSTAL_')]\n",
    "dpd_features = [c for c in inst_features if 'DPD' in c or 'DBD' in c]\n",
    "print(f'   {len(inst_features)} features (dont {len(dpd_features)} DPD/DBD)')\n",
    "\n",
    "print('\\n8. Credit Card :')\n",
    "cc_features = [c for c in train_df.columns if c.startswith('CC_')]\n",
    "print(f'   {len(cc_features)} features')\n",
    "\n",
    "print(f'\\nTOTAL NOUVELLES FEATURES : {len(new_features)}')\n",
    "print(f'COLONNES TOTALES : {train_df.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
